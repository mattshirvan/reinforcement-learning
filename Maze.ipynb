{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dyna Maze\n",
    "- Actions: {up, down, right, left} (Deterministic)\n",
    "- States: Maze Cells\n",
    "- Rewards: [0, 1]\n",
    "- Environment: Deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes \n",
    "episodes = 50\n",
    "\n",
    "# Start\n",
    "start = (2, 0)\n",
    "\n",
    "# Goal\n",
    "goal = (0, 8)\n",
    "\n",
    "# alpha \n",
    "alpha = 0.1\n",
    "\n",
    "# n-planning\n",
    "n = [0, 5, 50]\n",
    "\n",
    "# gamma \n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create():\n",
    "    \"\"\"\n",
    "    Create and return Q and arbitrary policy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create length and width to iterate grid\n",
    "    length = 9; width = 6\n",
    "\n",
    "    # Set of equiprobable actions N, S, E, W or Up, Down, Right, Left\n",
    "    actions = [(-1, 0), (1, 0), (0, 1), (0, -1)]\n",
    "    \n",
    "    # Policy\n",
    "    policy = {}; Q = {}\n",
    "\n",
    "    # Create states for policy\n",
    "    for i in range(length*width):\n",
    "\n",
    "        # Create row index\n",
    "        row = i // length\n",
    "\n",
    "        # Create column index\n",
    "        column = i % length\n",
    "\n",
    "        # Create policy\n",
    "        policy[(row, column)] = actions\n",
    "        \n",
    "        # Create Q(s,a)\n",
    "        Q[(row, column)] = [0, 0, 0, 0]\n",
    "        \n",
    "    # Q and policy \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary(state):\n",
    "    \"\"\"\n",
    "    Returns if agent is in bounds\n",
    "    \"\"\"\n",
    "    \n",
    "    # row boundary\n",
    "    row_boundary = (state[0] < 0 or state[0] > 5) \n",
    "        \n",
    "    # column boundary\n",
    "    column_boundary = (state[1] < 0 or state[1] > 8) \n",
    "    \n",
    "    # Obstacle\n",
    "    obstacle = state in [(1, 2), (2, 2), (3, 2), (4, 5), (0, 7), (1, 7), (2, 7)]\n",
    "    \n",
    "    # if not in bounds\n",
    "    return row_boundary or column_boundary or obstacle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(Q, s):\n",
    "    \"\"\"\n",
    "    Return the argmax Q(S',a)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keep track of ties\n",
    "    ties = []\n",
    "    \n",
    "    # loop through Q(S', a)\n",
    "    for a in Q[s]:\n",
    "        \n",
    "        # Check if tie \n",
    "        if a == max(Q[s]):\n",
    "            \n",
    "            # tie found\n",
    "            ties.append(a)\n",
    "            \n",
    "    return random.choice(ties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(Q, policy, state, goal):\n",
    "    \"\"\"\n",
    "    Return an action based on e-greedy policy\n",
    "    \"\"\"\n",
    "\n",
    "    # e-greedy action\n",
    "    action = argmax(Q, state)\n",
    "    \n",
    "    # Exploration\n",
    "    if random.random() < 0.1:\n",
    "        \n",
    "        # Explorative action\n",
    "        action = random.choice(policy[state])\n",
    "    \n",
    "    # e-greedy action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action, goal):\n",
    "    \"\"\"\n",
    "    Return environment obervation\n",
    "    \"\"\"\n",
    "    \n",
    "    # S' after a\n",
    "    s_prime = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "    # Check goal reached\n",
    "    if s_prime == goal:\n",
    "        \n",
    "        # S', R = 1 on termination\n",
    "        return s_prime, 1\n",
    "    \n",
    "    # check if out of bounds\n",
    "    if boundary(s_prime):\n",
    "        \n",
    "        # Remain in state\n",
    "        s_prime = state\n",
    "    \n",
    "    # S', R'\n",
    "    return s_prime, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment(model, Q, policy, alpha, gamma, start, goal):\n",
    "    \"\"\"\n",
    "    Observation of 'real' experience and step counts returned by environment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize steps and counts\n",
    "    N = {state: 0 for state in policy.keys()}; visits = 0\n",
    "    \n",
    "    # S current (nonterminal) state\n",
    "    S = start\n",
    "\n",
    "    # One step planning for model\n",
    "    while S != goal:\n",
    "        \n",
    "        # Increment counts\n",
    "        N[S] += 1; visits += 1\n",
    "        \n",
    "        # A \"-greedy(S, Q)\n",
    "        A = e_greedy(Q, policy, S, goal)\n",
    "\n",
    "        # Take action A; observe resultant reward, R, and state, S'\n",
    "        s_prime, R = move(S, A, goal)\n",
    "\n",
    "        # Q(S, A) <-- Q(S, A) + a[R + gamma*max_a Q(S',a) - Q(S, A)]\n",
    "        Q[S][policy[S].index(A)] = Q[S][policy[S].index(A)] + alpha*((R + gamma*max(Q[s_prime])) - Q[S][policy[S].index(A)]) \n",
    "\n",
    "        # Model(S, A) <-- R, S' (assuming deterministic environment)\n",
    "        model[(S, A)] = (R, s_prime)\n",
    "\n",
    "        # S <-- s\n",
    "        S = s_prime\n",
    "    \n",
    "    # return Model(s, a)\n",
    "    return Q, model, N, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyna(alpha, n, episodes, start, goal):\n",
    "    \"\"\"\n",
    "    Return Tabular Dyna-Q\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Q(s, a) and Model(s, a) for all s of S and a of A(s)\n",
    "    Q, policy = create(); model = {}\n",
    "    \n",
    "    # Loop forever:\n",
    "    for _ in range(episodes):\n",
    "        \n",
    "        # Environment\n",
    "        Q, model, N, visits = environment(model, Q, policy, alpha, gamma, start, goal)\n",
    "        \n",
    "        # Loop repeat n times:\n",
    "        for i in range(n):\n",
    "            \n",
    "            # S <-- random  previously observed state and a random action A previously taken in S\n",
    "            S, A = random.choice(list(model.keys()))\n",
    "            \n",
    "            # R, S' <-- Model(S, A)\n",
    "            R, s_prime = model[(S,A)]\n",
    "            \n",
    "            # Q(S, A) <-- Q(S, A) + a[R + gamma*max_a Q(S',a) - Q(S, A)]\n",
    "            Q[S][policy[S].index(A)] = Q[S][policy[S].index(A)] + alpha*((R + gamma*max(Q[s_prime])) - Q[S][policy[S].index(A)]) \n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-600938ac6ab3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Show agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Agent {i+1}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdyna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-1bca52268a29>\u001b[0m in \u001b[0;36mdyna\u001b[1;34m(alpha, n, episodes, start, goal)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# Environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Loop repeat n times:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-6dba74a0b223>\u001b[0m in \u001b[0;36menvironment\u001b[1;34m(model, Q, policy, alpha, gamma, start, goal)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Take action A; observe resultant reward, R, and state, S'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0ms_prime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Q(S, A) <-- Q(S, A) + a[R + gamma*max_a Q(S',a) - Q(S, A)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-3d015e888424>\u001b[0m in \u001b[0;36mmove\u001b[1;34m(state, action, goal)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# S' after a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0ms_prime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Check goal reached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# create terminal printer instance\n",
    "pp = pprint.PrettyPrinter(width=160, compact=True)\n",
    "\n",
    "# Run Dyna-Q\n",
    "for i in range(len(n)):\n",
    "    \n",
    "    # Show agents\n",
    "    print(f\"Agent {i+1}\")\n",
    "    pp.pprint(dyna(alpha, n[i], episodes, start, goal))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
