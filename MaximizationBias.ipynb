{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximization Bias\n",
    "- States: Terminal 1 <-> B <-> A <-> Terminal 2 \n",
    "- Actions: Left, Right\n",
    "- Rewards: 0 except ending in Terminal 1 reward drawn from a normal distribution with mean 0.1 and variance 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon\n",
    "epsilon = 0.1\n",
    "\n",
    "# Episodes\n",
    "episodes = 300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(Q1, Q2, policy, state, epsilon):\n",
    "    \"\"\"\n",
    "    Return an action based on e-greedy policy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exploitation Arg max a Q2\n",
    "    argmax = Q2[state].index(max(Q2[state]))\n",
    "    \n",
    "    # Q1 > Q2\n",
    "    if max(Q1[state]) > max(Q2[state]):\n",
    "        \n",
    "    \n",
    "        # Exploitation Arg max a\n",
    "        argmax = Q1[state].index(max(Q1[state]))\n",
    "    \n",
    "    # e-greedy action\n",
    "    action = policy[state][argmax]\n",
    "    \n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        \n",
    "        # Explorative action\n",
    "        action = random.choice(policy[state])\n",
    "    \n",
    "    # e-greedy action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action):\n",
    "    \"\"\"\n",
    "    Return environment obervation\n",
    "    \"\"\"\n",
    "\n",
    "    # S' after \n",
    "    s_prime = state + action\n",
    "    \n",
    "    # Check if terminal one reached\n",
    "    if s_prime == 0:\n",
    "        \n",
    "        # S', R = uniform distribution on termination\n",
    "        return s_prime, random.gauss(-0.1, 1.0)\n",
    "    \n",
    "    # S', R = 0 on all transitions and terminal two\n",
    "    return s_prime, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_Q_learning(episodes, epsilon):\n",
    "    \"\"\"\n",
    "    Return q* from Q-Learning Off-Policy TD Control (e-greedy)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################\n",
    "    # Double Q-Learning for estimating Q = q* #\n",
    "    ###########################################\n",
    "    \n",
    "    # Policy and Terminal\n",
    "    policy = {0: [0, 0], 1: [-1, 1], 2: [-1, 1], 3: [0, 0]}; terminal = [0,3]\n",
    "    \n",
    "    # Algorithm parameters: step size a -> (0, 1], small \"a > 0\n",
    "    alpha = 0.1\n",
    "\n",
    "    # Initialize Q(s, a), for all s of S+, a of A(s), arbitrarily except that Q(terminal, Â·)=0\n",
    "    Q1 = {0: [0, 0], 1: [0, 0], 2: [0, 0], 3: [0, 0]}; Q2 = {0: [0, 0], 1: [0, 0], 2: [0, 0], 3: [0, 0]} \n",
    "    \n",
    "    #Loop for each episode:\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        S = 2\n",
    "\n",
    "        #Loop for each step of episode: until S is terminal\n",
    "        while S not in terminal:\n",
    "            \n",
    "            # Choose A from S using policy derived from Q1 & Q2 (e-greedy)\n",
    "            A = e_greedy(Q1, Q2, policy, S, epsilon)\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            S_prime, R = move(S, A)\n",
    "            \n",
    "            # With 0.5 probabilility\n",
    "            if random.choice((0, 1)) == 1:\n",
    "                \n",
    "                # Q1(S, A) <-- Q1(S, A) + [R + gamma*Q2(S', argmax Q1(S', a)) - Q1(S, A)]\n",
    "                Q1[S][policy[S].index(A)] = Q1[S][policy[S].index(A)] + alpha*(R + Q2[S_prime][Q1[S_prime].index(max(Q1[S_prime]))]  - Q1[S][policy[S].index(A)])\n",
    "            \n",
    "            # else:\n",
    "            else:\n",
    "                \n",
    "                # Q2(S, A) <-- Q2(S, A) + [R + gamma*Q1(S', argmax Q2(S', a)) - Q2(S, A)]\n",
    "                Q2[S][policy[S].index(A)] = Q2[S][policy[S].index(A)] + alpha*(R + Q1[S_prime][Q2[S_prime].index(max(Q2[S_prime]))] - Q2[S][policy[S].index(A)])\n",
    "                \n",
    "            # S <-- S'\n",
    "            S = S_prime\n",
    "            \n",
    "    # Output Q estimate of q*\n",
    "    return list(Q1.values()), list(Q2.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [-0.1872525738735224, 0.0], [-0.017257507212251496, 0.0], [0, 0]]\n",
      "\n",
      "[[0, 0], [-0.26303165999468825, 0.0], [-0.004797619023098102, 0.0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "Q1, Q2 = double_Q_learning(episodes, epsilon)\n",
    "print(Q1)\n",
    "print()\n",
    "print(Q2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
