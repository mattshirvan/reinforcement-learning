{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Jack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card:\n",
    "    \n",
    "    def __init__(self, rank, face, value, suit):\n",
    "        self.rank = rank\n",
    "        self.face = face\n",
    "        self.value = value\n",
    "        self.suit = suit\n",
    "\n",
    "    def get_rank(self):\n",
    "        return self.rank\n",
    "    \n",
    "    def get_face(self):\n",
    "        return self.face\n",
    "    \n",
    "    def get_value(self):\n",
    "        return self.value\n",
    "    \n",
    "    def get_suit(self):\n",
    "        return self.suit\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.value}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.face is not None:\n",
    "            return f\"{self.face} of {self.suit}, Value: {self.value}\"\n",
    "        return f\"{self.rank} of {self.suit}, Value: {self.value}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deck:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.value = 11\n",
    "        self.faces = {1: \"Ace\", 11: \"Jack\", 12: \"Queen\", 13: \"King\"}\n",
    "        self.ranks = [i for i in range(1, 14)]\n",
    "        self.suits = [\"Hearts\",\"Diamonds\", \"Clubs\", \"Spades\"]\n",
    "            \n",
    "    def card(self):\n",
    "        \"\"\"\n",
    "        Create and return card from infinite deck with replacement\n",
    "        \"\"\"\n",
    "        import random as rn\n",
    "        \n",
    "        rank = rn.choice(self.ranks)\n",
    "        face = None\n",
    "        value = rank\n",
    "        suit = rn.choice(self.suits)\n",
    "        \n",
    "        if rank == 1:\n",
    "            value = self.value\n",
    "            face = self.faces[rank]\n",
    "        \n",
    "        elif rank > 10:\n",
    "            value = self.value - 1\n",
    "            face = self.faces[rank]\n",
    "        \n",
    "        return Card(rank, face, value, suit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dealer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.deck = Deck()\n",
    "        self.hand = self.deal()\n",
    "        self.ace_positions = []\n",
    "        self.current_score = 0\n",
    "        self.ace_in_hand = False\n",
    "        self.soft = True\n",
    "        self.player_wins = False\n",
    "        self.player_busts = False\n",
    "        self.player_sticks = False\n",
    "        self.player_loses = self.player_busts == True\n",
    "        \n",
    "    def deal(self):\n",
    "        \"\"\"\n",
    "        First deal of game, returns agent's hand\n",
    "        \"\"\"\n",
    "        # deal dealer's hand\n",
    "        self.hand = [self.deck.card() for i in range(2)]\n",
    "        \n",
    "        # deal agent's hand\n",
    "        player = [self.deck.card() for i in range(2)]\n",
    "        \n",
    "        return player\n",
    "    \n",
    "    def get_hand(self):\n",
    "        \"\"\"\n",
    "        Return's dealer's hand\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.hand\n",
    "        \n",
    "    def show(self):\n",
    "        \"\"\"\n",
    "        Returns dealer's show card\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.hand[0]\n",
    "    \n",
    "    def reveal(self):\n",
    "        \"\"\"\n",
    "        Reveals hidden card\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.hand[1]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets hand after game\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hand.clear()\n",
    "    \n",
    "    def hit(self):\n",
    "        \"\"\"\n",
    "        Hit and deal card to self\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hand.append(self.deck.card())\n",
    "        \n",
    "    def stick(self):\n",
    "        \"\"\"\n",
    "        Returns boolean to stay\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.score() >= 17\n",
    "        \n",
    "    def hit_player(self):\n",
    "        \"\"\"\n",
    "        Deal one card to player\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.deck.card()\n",
    "    \n",
    "    def useable(self):\n",
    "        \"\"\"\n",
    "        Return whether ace is useable\n",
    "        \"\"\"\n",
    "        return self.soft and self.current_score < 17\n",
    "    \n",
    "    def score(self):\n",
    "        \"\"\"\n",
    "        Calculate dealer's card sum\n",
    "        \"\"\"\n",
    "        \n",
    "        # reset to sum hand\n",
    "        total = 0\n",
    "        \n",
    "        # iterate through hand\n",
    "        for i in range(len(self.hand)):\n",
    "            \n",
    "            # sum hand\n",
    "            total += self.hand[i].get_value()\n",
    "            \n",
    "            # check if ace in hand\n",
    "            if self.hand[i].get_value() == 11:\n",
    "                \n",
    "                # ace found\n",
    "                self.ace_in_hand = True\n",
    "                \n",
    "                # keep track of position in hand\n",
    "                self.ace_positions.append(i)\n",
    "                \n",
    "        # keep track of running total\n",
    "        self.current_score = total\n",
    "        \n",
    "        # ace makes sum too large\n",
    "        if self.ace_in_hand and self.useable() and total > 21:\n",
    "            \n",
    "            # check if aces left\n",
    "            if len(self.ace_positions) > 0:\n",
    "                \n",
    "                # change ace value from 11 to 1\n",
    "                self.hand[self.ace_positions.pop()].value = 1\n",
    "            \n",
    "                # no lonrger soft/useable\n",
    "                self.soft = False\n",
    "            \n",
    "        return total\n",
    "    \n",
    "    def strategy(self):\n",
    "        \"\"\"\n",
    "        Dealer's game strategy\n",
    "        \"\"\"\n",
    "        \n",
    "        # player wins return to start a new episode\n",
    "        if self.player_wins:\n",
    "            return False\n",
    "        \n",
    "        # dealer busts return to start a new episode\n",
    "        if self.score() > 21:\n",
    "            self.player_wins = True\n",
    "            return False\n",
    "        \n",
    "        # dealer's turn\n",
    "        elif self.player_sticks:\n",
    "            \n",
    "            # hand sum 17 - 20\n",
    "            if self.stick():\n",
    "                return False\n",
    "            \n",
    "            # ace is useable/soft\n",
    "            elif self.useable():\n",
    "                return True\n",
    "            \n",
    "            # otherwise hit until at least 17\n",
    "            elif self.score() < 17:\n",
    "                return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hand = []\n",
    "        self.ace_in_hand = False\n",
    "        self.useable = False\n",
    "        self.action = 0\n",
    "        self.ace_positions = []\n",
    "        self.policy = self.create_policy()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset hand after game\n",
    "        \"\"\"\n",
    "        self.hand.clear()\n",
    "        \n",
    "    def create_policy(self):\n",
    "        \"\"\"\n",
    "        Creates agent's arbitrary policy \n",
    "        \"\"\"\n",
    "        \n",
    "        # tabular solutions method\n",
    "        policy = {}\n",
    "        \n",
    "        # agent hit for all sums under 20\n",
    "        for hand in range(12, 20):\n",
    "          \n",
    "            # create policy\n",
    "            for show in range(2, 12):\n",
    "                \n",
    "                # Non-Useable ace\n",
    "                policy[(hand, show, False)] = 1\n",
    "                \n",
    "                # Useable ace\n",
    "                policy[(hand, show, True)] = 1\n",
    "                \n",
    "                # Stick policy\n",
    "                if hand >= 19:\n",
    "                    \n",
    "                    # Non-Useable ace\n",
    "                    policy[(20, show, False)] = 0\n",
    "                \n",
    "                    # Useable ace\n",
    "                    policy[(20, show, True)] = 0\n",
    "                    \n",
    "                    # Non-Useable ace\n",
    "                    policy[(21, show, False)] = 0\n",
    "                \n",
    "                    # Useable ace\n",
    "                    policy[(21, show, True)] = 0\n",
    "                    \n",
    "                    # Non-Useable ace\n",
    "                    policy[(22, show, False)] = 0\n",
    "                \n",
    "                    # Useable ace\n",
    "                    policy[(22, show, True)] = 0\n",
    "            \n",
    "        return policy\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"\n",
    "        Returns agent's policy\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.policy\n",
    "    \n",
    "    def get_hand(self):\n",
    "        \"\"\"\n",
    "        Returns agent's hand\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.hand\n",
    "    \n",
    "    def set_hand(self, hand):\n",
    "        \"\"\"\n",
    "        Dealer's initial deal\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hand = hand\n",
    "    \n",
    "    def hit(self, card):\n",
    "        \"\"\"\n",
    "        Add's hit card to hand\n",
    "        \"\"\"\n",
    "        \n",
    "        self.hand.append(card)\n",
    "    \n",
    "    def score(self):\n",
    "        \"\"\"\n",
    "        Calculate and return agent's sum of cards\n",
    "        \"\"\"\n",
    "        \n",
    "        # reset to keep track of current sum \n",
    "        total = 0\n",
    "        \n",
    "        # sum hand and keep track of aces\n",
    "        for i in range(len(self.hand)):\n",
    "            \n",
    "            # sum hand\n",
    "            total += self.hand[i].get_value()\n",
    "            \n",
    "            # track aces in hand\n",
    "            if self.hand[i].get_value() == 11:\n",
    "                \n",
    "                # ace found\n",
    "                self.ace_in_hand = True\n",
    "                self.useable = True\n",
    "                \n",
    "                # keep track of position in hand\n",
    "                self.ace_positions.append(i)\n",
    "        \n",
    "        # utility condition for agent's policy\n",
    "        if self.ace_in_hand and self.useable and total > 21:\n",
    "            \n",
    "            # check if aces left\n",
    "            if len(self.ace_positions) > 0:\n",
    "                \n",
    "                # change ace value from 11 to 1\n",
    "                self.hand[self.ace_positions.pop(0)].value = 1\n",
    "            \n",
    "                # ace is no longer useable\n",
    "                self.useable = False\n",
    "              \n",
    "                \n",
    "        return total\n",
    "    \n",
    "    def is_useable(self):\n",
    "        \"\"\"\n",
    "        Checks and returns if Ace is useable\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.ace_in_hand and self.useable\n",
    "    \n",
    "    def decision(self, score, show_card, useable):\n",
    "        \"\"\"\n",
    "        Returns agent's decision: score, show card \n",
    "        \"\"\"\n",
    "        \n",
    "        return self.policy[(score, show_card, useable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.player_turn = True\n",
    "        self.dealer_turn = False\n",
    "        self.game_over = False\n",
    "        self.episode = []\n",
    "        self.reward = 0\n",
    "        self.score = 0\n",
    "        self.start = True\n",
    "        self.win = 21        \n",
    "        self.agent = Agent()\n",
    "        self.dealer = Dealer()\n",
    "        \n",
    "    def on(self, initial_state = None):\n",
    "        \"\"\"\n",
    "        Game Environment\n",
    "        Returns Episode: (state, action, reward)\n",
    "        \"\"\"\n",
    "        \n",
    "        # GAME LOOP\n",
    "        while not self.game_over:\n",
    "            \n",
    "            # initial start of game\n",
    "            if self.start:\n",
    "                \n",
    "                # MC First Visit - first deal\n",
    "                self.agent.set_hand(self.dealer.deal())\n",
    "                \n",
    "                # Check for MC Exploration Starts\n",
    "                if initial_state is not None:\n",
    "                    \n",
    "                    # first deal\n",
    "                    self.agent.set_hand(initial_state)\n",
    "\n",
    "                # Check if natural\n",
    "                if self.agent.score() == self.win:\n",
    "                    \n",
    "                    # GAME OVER\n",
    "                    self.dealer.player_wins = True\n",
    "                    \n",
    "                    # return episode\n",
    "                    self.episode.append(((self.agent.score(), self.dealer.show().get_value(), self.agent.is_useable()), self.agent.action, self.reward))\n",
    "                    return self.episode\n",
    "                \n",
    "                # game in play\n",
    "                self.start = False\n",
    "                \n",
    "            # Agent's turn\n",
    "            if self.player_turn:\n",
    "                \n",
    "                # keep track of current score\n",
    "                self.score = self.agent.score()\n",
    "                \n",
    "                # Check if 21 reached\n",
    "                if self.score == self.win:\n",
    "                    \n",
    "                    # Agent's turn over\n",
    "                    self.player_turn = False\n",
    "                    \n",
    "                    # Dealer's turn\n",
    "                    self.dealer_turn = True\n",
    "                    \n",
    "                # check if agent busts\n",
    "                elif self.score > self.win:\n",
    "                    \n",
    "                    # Agent's turn over\n",
    "                    self.player_turn = False\n",
    "                    \n",
    "                    # Dealer's turn\n",
    "                    self.dealer_turn = False\n",
    "                    \n",
    "                    # Agent loses\n",
    "                    self.dealer.player_busts = True\n",
    "                    \n",
    "                    # Game Over\n",
    "                    self.game_over = True\n",
    "                    \n",
    "                    # set score to 22 for bust\n",
    "                    self.score = 22\n",
    "                    \n",
    "                    # Consequence to increase regret\n",
    "                    self.reward = -1\n",
    "                \n",
    "                # check agent sticks\n",
    "                elif self.agent.decision(self.score, self.dealer.show().get_value(), self.agent.is_useable()) == 0:\n",
    "                    \n",
    "                    # Agent's turn over\n",
    "                    self.player_turn = False; self.dealer.player_sticks = True\n",
    "                    \n",
    "                    # Dealer's turn\n",
    "                    self.dealer_turn = True\n",
    "                    \n",
    "                    # set action\n",
    "                    self.agent.action = 0\n",
    "                    \n",
    "                # otherwise hit\n",
    "                elif self.agent.decision(self.score, self.dealer.show().get_value(), self.agent.is_useable()) == 1:\n",
    "                    \n",
    "                    # add card to hand\n",
    "                    self.agent.hit(self.dealer.hit_player())\n",
    "                    \n",
    "                    # set action\n",
    "                    self.agent.action = 1\n",
    "                \n",
    "                # Add episode\n",
    "                if self.score >= 12 and self.score < 22:\n",
    "                    \n",
    "                    # Episode following pi: S0, A0, R1, ..., St-1, At-1, Rt\n",
    "                    self.episode.append(((self.score, self.dealer.show().get_value(), self.agent.is_useable()), self.agent.action, self.reward))\n",
    "                \n",
    "            # Dealer's turn\n",
    "            if self.dealer_turn and not self.game_over:\n",
    "                \n",
    "                # keep track of dealers score\n",
    "                dealer_score = self.dealer.score()\n",
    "                \n",
    "                # Dealer's strategy\n",
    "                if self.dealer.strategy():\n",
    "                    self.dealer.hit()\n",
    "                    \n",
    "                # Dealer's turn over\n",
    "                else:\n",
    "                    \n",
    "                    # Dealer's turn\n",
    "                    self.dealer_turn = False\n",
    "                    \n",
    "                    # game ends\n",
    "                    self.game_over = True\n",
    "                    \n",
    "                    # if dealer busts\n",
    "                    if dealer_score > self.win:\n",
    "                        \n",
    "                        # reset for terminal state\n",
    "                        dealer_score = 0\n",
    "                    \n",
    "                    # State\n",
    "                    state = (self.score, self.dealer.show().get_value(), self.agent.is_useable())\n",
    "                    \n",
    "                    # Reward \n",
    "                    self.reward = self.terminal(dealer_score, self.score)\n",
    "                    \n",
    "                    # Episode following pi: S0, A0, R1, ..., St-1, At-1, Rt \n",
    "                    self.episode.append((state, self.agent.action, self.reward))\n",
    "                    \n",
    "            # GAME OVER\n",
    "            if self.game_over:\n",
    "\n",
    "                # return episode\n",
    "                return self.episode[::-1]\n",
    "            \n",
    "        \n",
    "    def terminal(self, dealer_score, agent_score):\n",
    "        \"\"\"\n",
    "        Terminal state\n",
    "        Returns reward: [-1, 0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Lose\n",
    "        if dealer_score > agent_score or agent_score > self.win:\n",
    "            return -1\n",
    "        \n",
    "        # Draw\n",
    "        if dealer_score == agent_score:\n",
    "            return 0\n",
    "        \n",
    "        # Win\n",
    "        if dealer_score < agent_score:\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.game = Game()\n",
    "        self.policy = self.game.agent.get_policy()\n",
    "        self.V = {}\n",
    "        self.Returns = {}\n",
    "        self.Q = {}\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 0.01\n",
    "        self.probability = {}\n",
    "        self.appears = []\n",
    "        self.G = 0\n",
    "        self.S = 0\n",
    "        self.A = 1\n",
    "        self.R = 2\n",
    "        self.W = 1\n",
    "        \n",
    "    def average(self, returns):\n",
    "        \"\"\"\n",
    "        Returns average(Returns(St, At))\n",
    "        \"\"\"\n",
    "        \n",
    "        return sum(returns)/len(returns)\n",
    "    \n",
    "    def first_visit(self, episodes):\n",
    "        \"\"\"\n",
    "        First-visit MC prediction, for estimating V ⇡ v⇡\n",
    "        \"\"\"\n",
    "        \n",
    "        #########\n",
    "        # Input #\n",
    "        #########\n",
    "        \n",
    "        # Input Policy\n",
    "        self.policy = self.game.agent.get_policy()\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "        \n",
    "        # Initialize V(s) and Returns(s) arbitrarily\n",
    "        for state in self.policy: \n",
    "        \n",
    "            # State-Value Function V(s)\n",
    "            self.V[state] = 0.0\n",
    "\n",
    "            # Returns(St)\n",
    "            self.Returns[state] = []\n",
    "        \n",
    "        ################\n",
    "        # Loop Forever #\n",
    "        ################\n",
    "        \n",
    "        # Loop for each episode\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT\n",
    "            self.game = Game(); episode = self.game.on()\n",
    "            \n",
    "            # G <- 0\n",
    "            self.G = 0\n",
    "            \n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            for step in episode:\n",
    "                \n",
    "                # G <- Gamma*G + Rt+1\n",
    "                self.G = self.gamma*self.G + step[self.R]\n",
    "                \n",
    "                if step[self.S] not in self.appears:\n",
    "              \n",
    "                    # Append G to Returns(St)\n",
    "                    self.Returns[step[self.S]].append(self.G)\n",
    "\n",
    "                    # V(St) <- average(Returns(St))\n",
    "                    self.V[step[self.S]] = self.average(self.Returns[step[self.S]])\n",
    "                    \n",
    "                    # state appeared\n",
    "                    self.appears.append(step[self.S])\n",
    "\n",
    "        # V = v_pi\n",
    "        return self.V\n",
    "    \n",
    "    \n",
    "    def exploration_starts(self, episodes):\n",
    "        \"\"\"\n",
    "        Monte Carlo ES (Exploring Starts), for estimating Pi = pi*\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "          \n",
    "        # arbitrary policy\n",
    "        policy = self.policy.copy()\n",
    "        \n",
    "        # Initialize Q(s,a) and Returns(s,a) arbitrarily\n",
    "        for state in policy: \n",
    "        \n",
    "            # State-Action-Value Function Q(s,a)\n",
    "            self.Q[(state, 0)] = 0.0\n",
    "            self.Q[(state, 1)] = 0.0\n",
    "\n",
    "            # Returns(s,a)\n",
    "            self.Returns[(state, 0)] = []\n",
    "            self.Returns[(state, 1)] = []\n",
    "        \n",
    "        ################\n",
    "        # Loop Forever #\n",
    "        ################\n",
    "        \n",
    "        # Loop for each episode\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT\n",
    "            self.game = Game(); episode = self.game.on(initial_state = self.game.dealer.deal())\n",
    "            \n",
    "            # G <- 0\n",
    "            self.G = 0\n",
    "            \n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            for step in episode:\n",
    "                \n",
    "                # G <- Gamma*G + Rt+1\n",
    "                self.G = self.gamma*self.G + step[self.R]\n",
    "              \n",
    "                # Append G to Returns(St, At)\n",
    "                self.Returns[(step[self.S], step[self.A])].append(self.G)\n",
    "\n",
    "                # Q(St, At) <- average(Returns(St, At))\n",
    "                self.Q[(step[self.S], step[self.A])] = self.average(self.Returns[(step[self.S], step[self.A])])\n",
    "                \n",
    "                # pi⇡(St) argmax a Q(St, a)\n",
    "                self.policy[step[self.S]] = [state[1] for state, value in self.Q.items() if value == max(self.Q[step[self.S], 0], self.Q[step[self.S], 1])][0]\n",
    "                \n",
    "        # Pi = pi*\n",
    "        return self.policy\n",
    "    \n",
    "    def on_policy(self):\n",
    "        \"\"\"\n",
    "        On-policy first-visit MC control (for \"e-soft policies)\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "          \n",
    "        # arbitrary policy\n",
    "        policy = self.policy.copy()\n",
    "        \n",
    "        # Initialize Q(s,a) and Returns(s,a) arbitrarily\n",
    "        for state in policy: \n",
    "        \n",
    "            # State-Action-Value Function Q(s,a)\n",
    "            self.Q[(state, 0)] = 0.0\n",
    "            self.Q[(state, 1)] = 0.0\n",
    "\n",
    "            # Returns(s,a)\n",
    "            self.Returns[(state, 0)] = []\n",
    "            self.Returns[(state, 1)] = []\n",
    "        \n",
    "        ################\n",
    "        # Loop Forever #\n",
    "        ################\n",
    "        \n",
    "        # Loop for each episode\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT\n",
    "            self.game = Game(); episode = self.game.on()\n",
    "            \n",
    "            # G <- 0\n",
    "            self.G = 0\n",
    "            \n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            for step in episode:\n",
    "                \n",
    "                # G <- Gamma*G + Rt+1\n",
    "                self.G = self.gamma*self.G + step[self.R]\n",
    "              \n",
    "                # Append G to Returns(St, At)\n",
    "                self.Returns[(step[self.S], step[self.A])].append(self.G)\n",
    "\n",
    "                # Q(St, At) <- average(Returns(St, At))\n",
    "                self.Q[(step[self.S], step[self.A])] = self.average(self.Returns[(step[self.S], step[self.A])])\n",
    "                \n",
    "                # pi⇡(St) argmax a Q(St, a)\n",
    "                A = [state[1] for state, value in self.Q.items() if value == max(self.Q[step[self.S], 0], self.Q[step[self.S], 1])][0]\n",
    "                \n",
    "                # For all a of A(St):\n",
    "                for a in self.policy.values():\n",
    "                    \n",
    "                    # pi(a|St) <-- 1 - e + e/|A(St)| if a = A* \n",
    "                    self.probability[step[self.S]] = 1 - self.epsilon + self.epsilon/len(list(self.policy.values()))\n",
    "                    \n",
    "                    # if a != A*\n",
    "                    if a != A:\n",
    "                        \n",
    "                        # pi(a|St) <-- e/|A(St)|\n",
    "                        self.probability[step[self.S]] = self.epsilon/len(list(self.policy.values()))\n",
    "                    \n",
    "        # Pi = pi*\n",
    "        return self.policy\n",
    "    \n",
    "    def off_policy(self):\n",
    "        \"\"\"\n",
    "        Off-policy MC prediction (policy evaluation) for estimating Q = q pi\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "          \n",
    "        # arbitrary policy\n",
    "        policy = self.policy.copy()\n",
    "        \n",
    "        # Initialize Q(s,a) and Returns(s,a) arbitrarily\n",
    "        for state in policy: \n",
    "        \n",
    "            # State-Action-Value Function Q(s,a)\n",
    "            self.Q[(state, 0)] = 0.0\n",
    "            self.Q[(state, 1)] = 0.0\n",
    "\n",
    "            # Returns(s,a)\n",
    "            self.C[(state, 0)] = 0\n",
    "            self.C[(state, 1)] = 0\n",
    "        \n",
    "        ################\n",
    "        # Loop Forever #\n",
    "        ################\n",
    "        \n",
    "        # Loop for each episode\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT\n",
    "            self.game = Game(); episode = self.game.on()\n",
    "            \n",
    "            # G <- 0\n",
    "            self.G = 0\n",
    "            \n",
    "            # W <- 0\n",
    "            self.W = 1\n",
    "            \n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            for step in episode:\n",
    "                \n",
    "                # while W != 0\n",
    "                while self.W != 0:\n",
    "                    \n",
    "                    # G <- Gamma*G + Rt+1\n",
    "                    self.G = self.gamma*self.G + step[self.R]\n",
    "\n",
    "                    # C(St, At) <-- C(St, At) + W\n",
    "                    self.C[(step[self.S], step[self.A])] += self.W\n",
    "\n",
    "                    # Q(St, At) <-- Q(St, At) + W/C(St, At)[G - Q(St, At)]  \n",
    "                    self.Q[(step[self.S], step[self.A])] = (self.W/self.C[(step[self.S], step[self.A])])*(self.G - self.Q[(step[self.S], step[self.A])])\n",
    "\n",
    "                    # W <-- W pi(a|s)/b(a|s)\n",
    "                    self.W *= (self.probability[step[self.S]]/0.5)\n",
    "        \n",
    "        # return Q estimate of q-pi\n",
    "        return self.Q\n",
    "    \n",
    "    def off_control(self):\n",
    "        \"\"\"\n",
    "        Off-policy MC control, for estimating pi = p⇡*\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "          \n",
    "        # pi(s) argmax a Q(s, a) (with ties broken consistently)\n",
    "        policy = self.policy.copy()\n",
    "        \n",
    "        # Initialize Q(s,a) and Returns(s,a) arbitrarily\n",
    "        for state in policy: \n",
    "        \n",
    "            # State-Action-Value Function Q(s,a)\n",
    "            self.Q[(state, 0)] = 0.0\n",
    "            self.Q[(state, 1)] = 0.0\n",
    "\n",
    "            # Returns(s,a)\n",
    "            self.C[(state, 0)] = 0\n",
    "            self.C[(state, 1)] = 0\n",
    "        \n",
    "        ################\n",
    "        # Loop Forever #\n",
    "        ################\n",
    "        \n",
    "        # Loop for each episode\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT\n",
    "            self.game = Game(); episode = self.game.on()\n",
    "            \n",
    "            # G <- 0\n",
    "            self.G = 0\n",
    "            \n",
    "            # W <- 0\n",
    "            self.W = 1\n",
    "            \n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0:\n",
    "            for step in episode:\n",
    " \n",
    "                # G <- Gamma*G + Rt+1\n",
    "                self.G = self.gamma*self.G + step[self.R]\n",
    "\n",
    "                # C(St, At) <-- C(St, At) + W\n",
    "                self.C[(step[self.S], step[self.A])] += self.W\n",
    "\n",
    "                # Q(St, At) <-- Q(St, At) + W/C(St, At)[G - Q(St, At)]  \n",
    "                self.Q[(step[self.S], step[self.A])] = (self.W/self.C[(step[self.S], step[self.A])])*(self.G - self.Q[(step[self.S], step[self.A])])\n",
    "\n",
    "                # pi(St) argmax a Q(St, a) (with ties broken consistently)\n",
    "                self.policy[step[self.S]] = [state[1] for state, value in self.Q.items() if value == max(self.Q[step[self.S], 0], self.Q[step[self.S], 1])][0]\n",
    "\n",
    "                # If At != pi(St) \n",
    "                if step[self.A] != self.policy[step[self.S]]:\n",
    "                    \n",
    "                    # then exit inner Loop (proceed to next episode)\n",
    "                    break\n",
    "\n",
    "                # W <-- W pi(a|s)/b(a|s)\n",
    "                self.W *= (1/0.5)\n",
    "        \n",
    "        # return Q estimate of q-pi\n",
    "        return self.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Visit Episodes: 10,000\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(9, 4, False)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ef180c2e3571>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"First Visit Episodes: 10,000\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst_visit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mpp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-fa6749c6281e>\u001b[0m in \u001b[0;36mfirst_visit\u001b[1;34m(self, episodes)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# G <- 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5e874487f632>\u001b[0m in \u001b[0;36mon\u001b[1;34m(self, initial_state)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;31m# check agent sticks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdealer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_useable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                     \u001b[1;31m# Agent's turn over\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bbad8579a91f>\u001b[0m in \u001b[0;36mdecision\u001b[1;34m(self, score, show_card, useable)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \"\"\"\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_card\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0museable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: (9, 4, False)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import pprint\n",
    "    \n",
    "    # create terminal printer instance\n",
    "    pp = pprint.PrettyPrinter(width=160, compact=True)\n",
    "    \n",
    "    # Monte Carlo Algorithms\n",
    "    mc = MonteCarlo()\n",
    "    game = Game()\n",
    "    \n",
    "    print(\"First Visit Episodes: 10,000\")\n",
    "    pp.pprint(mc.first_visit(10000))\n",
    "    pp.pprint(game.on())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
