{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrates difference between MC First Visit and $\\alpha$ MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: [0, -2.0, -1.0, -2.0, 0]\n",
      "V2: [0, 0.9, 2.1, 0.9, 0]\n",
      "V3: [0, 0.0, 3.0, 0.0, 0]\n",
      "V4: [0, 0.9, 2.0100000000000002, 0.9, 0]\n"
     ]
    }
   ],
   "source": [
    "# V(St) \n",
    "V = [0, 1, 2, 1, 0]; V2 = [0, 1, 2, 1, 0]; V3 = [0, 1, 2, 1, 0]; V4 = [0, 1, 2, 1, 0]\n",
    "\n",
    "# Returns(St)\n",
    "returns = [[] for i in range(len(V))]; N = [0] * len(V)\n",
    "\n",
    "# alpha\n",
    "a = 0.1\n",
    "\n",
    "# One Episode\n",
    "episode = [(2, \"left\", -1), (1, \"right\", -1), (2, \"right\", -1), (3, \"right\", +1)]\n",
    "\n",
    "# return\n",
    "G = 0\n",
    "\n",
    "# first-visit\n",
    "appears = []\n",
    "\n",
    "# loop for step of episode\n",
    "for S, A, R in episode:\n",
    "    \n",
    "    # G <-- gamma*G + Rt+1\n",
    "    G = G + R\n",
    "    \n",
    "    # State visit count\n",
    "    N[S] += 1\n",
    "    \n",
    "    ###############\n",
    "    # Constant-MC #\n",
    "    ###############\n",
    "    V4[S] = V4[S] + a*(G + V4[S])\n",
    "    \n",
    "    # unless St appears in S0, S1, S2, ... St-1\n",
    "    if S not in appears:\n",
    "        \n",
    "        # Append G to returns\n",
    "        returns[S].append(G)\n",
    "        \n",
    "        # State visited in episode\n",
    "        appears.append(S)\n",
    "        \n",
    "        ##################\n",
    "        # MC first visit #\n",
    "        ##################\n",
    "        V[S] = sum(returns[S])/N[S]\n",
    "        \n",
    "        ###############\n",
    "        # Constant-MC #\n",
    "        ###############\n",
    "        V2[S] = V2[S] + a*(G + V2[S])\n",
    "        \n",
    "        #####################\n",
    "        # alpha first visit #\n",
    "        #####################\n",
    "        V3[S] = V3[S] + 1/N[S]*(G + V3[S]) # no bootstrapping\n",
    "        \n",
    "print(f\"V: {V}\")\n",
    "print(f\"V2: {V2}\")\n",
    "print(f\"V3: {V3}\")\n",
    "print(f\"V4: {V4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld with TD(0) SARSA and MC display value functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(0): [0, -4.8, -2.0, -2, -3, -1.0, -3, -1, 0]\n",
      "{0: [0], 1: [-0.1], 2: [-0.1], 3: [0], 4: [0], 5: [-0.1], 6: [0], 7: [0], 8: [0]}\n",
      "{0: [0], 1: [-1.0], 2: [-2.0], 3: [0], 4: [0], 5: [-3.0], 6: [0], 7: [0], 8: [0]}\n"
     ]
    }
   ],
   "source": [
    "# Input pi, the policy to be evaluated\n",
    "policy = {1: 1, 2: 3, 3: -3, 4: 3, 5: 3, 6: -3, 7: 1}; episode = {1:1, 2:3, 5:3}\n",
    "\n",
    "# Initialize Q(s, a), for all s 2 S+,, a of A(s) arbitrarily except that V (terminal)=0\n",
    "v = [0, -5, -2, -2, -3, -1, -3, -1, 0]; Q = {i: [0] for i in range(9)}; G = 0; q = {i: [0] for i in range(9)}\n",
    "\n",
    "# appear\n",
    "appear = []\n",
    "\n",
    "# list of returns\n",
    "returns_mc = [[] for i in range(len(v))]\n",
    "\n",
    "# loop for each step of episode\n",
    "for state, action in episode.items():\n",
    "    \n",
    "    # Take A observe R, S'\n",
    "    reward = -1\n",
    "    \n",
    "    # Actual return at time t\n",
    "    G = G + reward\n",
    "    \n",
    "    # Q target\n",
    "    q_target = reward + Q[state+action][0]\n",
    "    \n",
    "    # V target\n",
    "    v_target = reward + v[state+action]\n",
    "    \n",
    "    ################\n",
    "    # SARSA Update #\n",
    "    ################\n",
    "    Q[state][0] = Q[state][0] + a*(q_target - Q[state][0])\n",
    "    \n",
    "    ################\n",
    "    # TD(0) Update #\n",
    "    ################\n",
    "    v[state] = v[state] + a*(v_target - v[state])\n",
    "    \n",
    "    # MC Method\n",
    "    if (state, action) not in appear:\n",
    "        \n",
    "        # s, a seen\n",
    "        appear.append((state, action))\n",
    "        \n",
    "        # Append G to returns\n",
    "        returns_mc[state].append(G)\n",
    "        \n",
    "        #############\n",
    "        # MC Update #\n",
    "        #############\n",
    "        q[state][0] = sum(returns_mc[state])/len(returns_mc[state])\n",
    "    \n",
    "print(f\"TD(0): {v}\")\n",
    "print(Q)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld with TD(0) SARSA and MC display value functions\n",
    "- Backwards from T -> 5 -> 2 -> 1 -> T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(0): [0, -4.6, -2.4, -2, -3, -1.2, -3, -1, 0]\n",
      "{0: [0], 1: [-0.1], 2: [-0.1], 3: [0], 4: [0], 5: [-0.1], 6: [0], 7: [0], 8: [0]}\n",
      "{0: [0], 1: [-3.0], 2: [-2.0], 3: [0], 4: [0], 5: [-1.0], 6: [0], 7: [0], 8: [0]}\n"
     ]
    }
   ],
   "source": [
    "# Input pi, the policy to be evaluated\n",
    "policy = {1: 1, 2: 3, 3: -3, 4: 3, 5: 3, 6: -3, 7: 1}; episode2 = {5:-3, 2:-1, 1:-1}\n",
    "\n",
    "# Initialize Q(s, a), for all s 2 S+,, a of A(s) arbitrarily except that V (terminal)=0\n",
    "v = [0, -5, -2, -2, -3, -1, -3, -1, 0]; Q = {i: [0] for i in range(9)}; G = 0; q = {i: [0] for i in range(9)}; N = [0] *len(v)\n",
    "\n",
    "# appear\n",
    "appear = []\n",
    "\n",
    "# list of returns\n",
    "returns_mc = [[] for i in range(len(v))]\n",
    "\n",
    "# loop for each step of episode\n",
    "for state, action in episode2.items():\n",
    "    \n",
    "    # Take A observe R, S'\n",
    "    reward = -1\n",
    "    \n",
    "    # Actual return at time t\n",
    "    G = G + reward\n",
    "    \n",
    "    # Q target\n",
    "    q_target = reward + Q[state+action][0]\n",
    "    \n",
    "    # V target\n",
    "    v_target = reward + v[state+action]\n",
    "    \n",
    "    # N(S) <-- +1\n",
    "    N[state] += 1\n",
    "    \n",
    "    ################\n",
    "    # SARSA Update #\n",
    "    ################\n",
    "    Q[state][0] = Q[state][0] + a*(q_target - Q[state][0])\n",
    "    \n",
    "    ################\n",
    "    # TD(0) Update #\n",
    "    ################\n",
    "    v[state] = v[state] + a*(v_target - v[state])\n",
    "    \n",
    "    # MC Method\n",
    "    if (state, action) not in appear:\n",
    "        \n",
    "        # s, a seen\n",
    "        appear.append((state, action))\n",
    "        \n",
    "        # Append G to returns\n",
    "        returns_mc[state].append(G)\n",
    "        \n",
    "        #############\n",
    "        # MC Update #\n",
    "        #############\n",
    "        q[state][0] = sum(returns_mc[state])/N[state]\n",
    "        \n",
    "print(f\"TD(0): {v}\")\n",
    "print(Q)\n",
    "print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
