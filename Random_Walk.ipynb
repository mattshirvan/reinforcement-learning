{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Simulation\n",
    "- Agent: Moving agent takes random walks in the environment. \n",
    "- Actions: N, S, E, W, NE, NW, SE, SW -> small or normal steps\n",
    "- State: State the agent is in -> taking an action in state -> the next state the agent will end up in.\n",
    "- Environment: Collection of states -> agent takes actions -> observations and rewards. \n",
    "- Episodes: 10,000 simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.actions = {\"N\": (-1, 0), \"S\": (1, 0), \"E\": (0, 1), \"W\": (0, -1), \\\n",
    "                        \"NE\": (-1, 1), \"SE\": (1, 1), \"NW\": (-1, -1), \"SW\": (1, -1)}\n",
    "        \n",
    "        self.small_actions = {\"N\": (-0.1, 0.0), \"S\": (0.1, 0.0), \"E\": (0.0, 0.1), \"W\": (0.0, -0.1), \\\n",
    "                              \"NE\": (-0.1, 0.1), \"SE\": (0.1, 0.1), \"NW\": (-0.1, -0.1), \"SW\": (0.1, -0.1)}\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Return a random step\n",
    "        \"\"\"\n",
    "        \n",
    "        return random.choice(list(self.actions.values()))\n",
    "        \n",
    "    def small_step(self):\n",
    "        \"\"\"\n",
    "        Return a random small step\n",
    "        \"\"\"\n",
    "        \n",
    "        return random.choice(list(self.small_actions.values()))\n",
    "\n",
    "    def move(self, path):\n",
    "        \"\"\"\n",
    "        Return a big or small step based on state's path size that agent encounters\n",
    "        \"\"\"\n",
    "        \n",
    "        # check path size for small step\n",
    "        if path >= 0 and path <= 0.5:\n",
    "            return self.small_step()\n",
    "        \n",
    "        # return normal step\n",
    "        return self.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self, x = 0, y = 0):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.path = random.uniform(-1, 3)\n",
    "\n",
    "    def action(self, move):\n",
    "        \"\"\"\n",
    "        Return a new location after agent's action (dx and dy)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Agent's action\n",
    "        dx, dy = move\n",
    "        \n",
    "        return State(self.x + dx, self.y + dy)\n",
    "    \n",
    "    def get_location(self):\n",
    "        \"\"\"\n",
    "        Return the agent's current (x, y) coordinates\n",
    "        \"\"\"\n",
    "        \n",
    "        return (self.x, self.y)\n",
    "    \n",
    "    def get_x(self):\n",
    "        \"\"\"\n",
    "        Return x coordinate\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.x\n",
    "    \n",
    "    def get_y(self):\n",
    "        \"\"\"\n",
    "        Return y coordinate\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.y\n",
    "    \n",
    "    def get_path(self):\n",
    "        \"\"\"\n",
    "        Return the size of the path from current state to next state\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.path\n",
    "    \n",
    "    def distance(self, next_state):\n",
    "        \"\"\"\n",
    "        Return distance to any state: utilizing Euclidean Distance\n",
    "        \"\"\"\n",
    "        \n",
    "        return ((self.x - next_state.get_x())**2 + (self.y - next_state.get_y())**2)**0.5\n",
    "    \n",
    "    def start_distance(self):\n",
    "        \"\"\"\n",
    "        Return distance from starting point\n",
    "        \"\"\"\n",
    "        \n",
    "        return abs(self.x) + abs(self.y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.x}, {self.y}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.agent = Agent()\n",
    "        self.start_state = self.current_state = State()\n",
    "        self.exploration_state = State(x = random.randint(0, 31), y = random.randint(0, 31))        \n",
    "        self.goal_state = State(x = random.randint(0, 31), y = random.randint(0, 31))\n",
    "        self.V = {}\n",
    "        self.locations = [self.start_state]\n",
    "        self.consequence = -1\n",
    "        self.reward_goal = 1\n",
    "    \n",
    "    def seen(self, state):\n",
    "        \"\"\"\n",
    "        Add's to the Agent's trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add current state to agent's history\n",
    "        self.locations.append(state)\n",
    "        \n",
    "    def rewards(self):\n",
    "        \"\"\"\n",
    "        Return reward of 1 or consequence of -1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Return reward if goal state reached\n",
    "        if self.current_state == self.goal_state:\n",
    "            return self.reward_goal\n",
    "        \n",
    "        # Consequence for not reaching goal state\n",
    "        return self.consequence\n",
    "        \n",
    "    def move(self, steps):\n",
    "        \"\"\"\n",
    "        Return tuple (start distance, goal distance, state, action, reward\n",
    "        \"\"\"\n",
    "        \n",
    "        # Keep track of state\n",
    "        state = self.current_state\n",
    "        \n",
    "        # Simulate walk based on number of steps\n",
    "        for _ in range(steps):\n",
    "            \n",
    "            # Take walk\n",
    "            walk = self.agent.move(self.current_state.get_path())\n",
    "            \n",
    "            # Get new state\n",
    "            self.current_state = state.action(walk)\n",
    "            \n",
    "            # Add state to seen locations\n",
    "            self.seen(self.current_state)\n",
    "        \n",
    "        return self.current_state.start_distance(), self.current_state.distance(self.goal_state), state, walk, self.rewards()\n",
    "    \n",
    "    def simulation_test(self, miles, simulations):\n",
    "        \"\"\"\n",
    "        Return distances after basic Monte Carlo simulation\n",
    "        \"\"\"\n",
    "        \n",
    "        # Environment makes up an n miles of walkable terrain\n",
    "        for mile in range(1, miles+1):\n",
    "            \n",
    "            # Number of walks that agent doesn't need rescue\n",
    "            no_rescue = 0\n",
    "            \n",
    "            # returns\n",
    "            returns = []\n",
    "            \n",
    "            # Loop for each episode\n",
    "            for runs in range(simulations):\n",
    "            \n",
    "                # Generate and episode (St, At, Rt+1) and distances from start and goal\n",
    "                start_distance, goal_distance, state, action, reward = self.move(mile)\n",
    "                \n",
    "                # Check if rescue is not needed \n",
    "                if state.start_distance() <= 4:\n",
    "                    \n",
    "                    # Increase safety count\n",
    "                    no_rescue += 1\n",
    "                \n",
    "                # Add St, At, Rt+1 to episode trajectory\n",
    "                returns.append((state, action, reward))\n",
    "                \n",
    "            # All distances of states from start and goal\n",
    "            safety_percentage = (no_rescue / simulations)*100\n",
    "            \n",
    "            # V(St) <-- average(Returns(St))\n",
    "            self.V[self.current_state] = safety_percentage\n",
    "            \n",
    "            # Print simulations\n",
    "            print(f\"Miles walked: {mile}  safety: {safety_percentage}%\")\n",
    "            \n",
    "        # return V\n",
    "        return self.V\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miles walked: 1  safety: 0.7100000000000001%\n",
      "Miles walked: 2  safety: 0.0%\n",
      "Miles walked: 3  safety: 0.0%\n",
      "Miles walked: 4  safety: 0.0%\n",
      "Miles walked: 5  safety: 0.0%\n",
      "Miles walked: 6  safety: 0.0%\n",
      "Miles walked: 7  safety: 0.0%\n",
      "Miles walked: 8  safety: 0.0%\n",
      "Miles walked: 9  safety: 0.0%\n",
      "Miles walked: 10  safety: 0.0%\n",
      "Miles walked: 11  safety: 0.0%\n",
      "Miles walked: 12  safety: 0.0%\n",
      "Miles walked: 13  safety: 0.0%\n",
      "Miles walked: 14  safety: 0.0%\n",
      "Miles walked: 15  safety: 0.0%\n",
      "Miles walked: 16  safety: 0.0%\n",
      "Miles walked: 17  safety: 0.0%\n",
      "Miles walked: 18  safety: 0.0%\n",
      "Miles walked: 19  safety: 0.0%\n",
      "Miles walked: 20  safety: 0.0%\n",
      "Miles walked: 21  safety: 0.0%\n",
      "Miles walked: 22  safety: 0.0%\n",
      "Miles walked: 23  safety: 0.0%\n",
      "Miles walked: 24  safety: 0.0%\n",
      "Miles walked: 25  safety: 0.0%\n",
      "Miles walked: 26  safety: 0.0%\n",
      "Miles walked: 27  safety: 0.0%\n",
      "Miles walked: 28  safety: 0.0%\n",
      "Miles walked: 29  safety: 0.0%\n",
      "Miles walked: 30  safety: 0.0%\n",
      "{-69.39999999999992, 63.20000000000002: 0.7100000000000001, -293.99999999999966, -139.6000000000001: 0.0, -368.4999999999991, -127.0000000000002: 0.0, -278.00000000000034, 60.2: 0.0, -173.80000000000018, -31.099999999999923: 0.0, -49.199999999999996, -107.1999999999999: 0.0, 111.40000000000003, 14.199999999999825: 0.0, 278.3999999999998, 35.79999999999984: 0.0, 211.50000000000074, -82.80000000000027: 0.0, 303.90000000000146, 5.00000000000019: 0.0, 269.90000000000236, -46.69999999999977: 0.0, 113.50000000000242, -78.19999999999965: 0.0, 83.40000000000222, -149.09999999999965: 0.0, -27.199999999997722, -116.49999999999974: 0.0, -284.3999999999978, 49.40000000000034: 0.0, -355.0999999999998, 154.90000000000055: 0.0, -379.0000000000016, 111.80000000000078: 0.0, -572.8000000000015, -94.19999999999929: 0.0, -470.6000000000024, -332.59999999999957: 0.0, -422.80000000000246, -450.1999999999997: 0.0, -494.7000000000013, -402.1999999999981: 0.0, -531.5000000000024, -315.3999999999952: 0.0, -311.6000000000013, -269.1999999999939: 0.0, -257.600000000003, -387.1999999999932: 0.0, -362.30000000000405, -556.599999999994: 0.0, -457.4000000000043, -450.099999999993: 0.0, -606.3000000000045, -415.1999999999937: 0.0, -700.2000000000047, -597.5999999999945: 0.0, -835.7000000000055, -469.39999999999486: 0.0, -886.0000000000076, -433.09999999999616: 0.0}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    e = Environment()\n",
    "    \n",
    "    print(e.simulation_test(30, 20000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
