{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World\n",
    "\n",
    "###### https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math\n",
    "from random import gauss, uniform, randint, random, choice\n",
    "import math\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    def __init__(self, grid_size, A, B, A_prime, B_prime, A_goal, B_goal, gamma):\n",
    "        self.grid_size = grid_size\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.A_prime = A_prime\n",
    "        self.B_prime = B_prime\n",
    "        self.A_goal = A_goal\n",
    "        self.B_goal = B_goal\n",
    "        self.gamma = gamma\n",
    "        self.terminal = []\n",
    "        self.grid_world = [[0.0 for j in range(self.grid_size)] for i in range(self.grid_size)]\n",
    "        self.directions = { 'east': (0, 1), 'west': (0, -1), 'north': (-1, 0), 'south': (1, 0)}\n",
    "        self.policy = {}\n",
    "        self.policy_values = {}\n",
    "        self.reward = 0\n",
    "        self.returns = []\n",
    "        self.alpha = 0\n",
    "        self.beta = 0\n",
    "        self.probability = 1 / len(self.directions)\n",
    "        self.explored = set()\n",
    "        self.start = None\n",
    "        self.actions = []\n",
    "        \n",
    "    def set_grid_size(self, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "    def get_grid_size(self):\n",
    "        return self.grid_size\n",
    "    \n",
    "    def set_A(self, A):\n",
    "        self.A = A\n",
    "        \n",
    "    def get_A(self):\n",
    "        return self.A\n",
    "    \n",
    "    def set_A_prime(self, A_prime):\n",
    "        self.A_prime = A_prime\n",
    "        \n",
    "    def get_A_prime(self):\n",
    "        return self.A_prime\n",
    "    \n",
    "    def set_B(self, B):\n",
    "        self.B = B\n",
    "        \n",
    "    def get_B(self):\n",
    "        return self.B\n",
    "    \n",
    "    def set_B_prime(self, B_prime):\n",
    "        self.B_prime = B_prime\n",
    "        \n",
    "    def get_B_prime(self):\n",
    "        return self.B_prime\n",
    "   \n",
    "    def set_A_goal(self, goal):\n",
    "        self.A_goal = goal\n",
    "        \n",
    "    def get_A_goal(self):\n",
    "        return self.A_goal\n",
    "    \n",
    "    def set_B_goal(self, goal):\n",
    "        self.B_goal = goal\n",
    "        \n",
    "    def get_B_goal(self):\n",
    "        return self.B_goal\n",
    "    \n",
    "    def set_gamma(self, gamma):\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def Gamma(self):\n",
    "        return self.gamma\n",
    "    \n",
    "    def set_terminal(self, terminal):\n",
    "        self.terminal = terminal\n",
    "    \n",
    "    def get_terminal(self):\n",
    "        return self.terminal\n",
    "    \n",
    "    def set_policy(self, policy):\n",
    "        self.policy = policy\n",
    "        \n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "    \n",
    "    def set_policy_values(self, policy_values):\n",
    "        self.policy_values = policy_values\n",
    "        \n",
    "    def get_policy_values(self):\n",
    "        return self.policy_values\n",
    "    \n",
    "    def set_world(self, grid_world):\n",
    "        self.grid_world = grid_world\n",
    "    \n",
    "    def get_world(self):\n",
    "        return self.grid_world\n",
    "    \n",
    "    def set_returns(self, returns):\n",
    "        self.returns += returns\n",
    "        \n",
    "    def get_returns(self):\n",
    "        return self.returns\n",
    "    \n",
    "    def set_alpha(self, alpha):\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def get_alpha(self):\n",
    "        return self.alpha\n",
    "    \n",
    "    def set_beta(self, beta):\n",
    "        self.beta = beta\n",
    "        \n",
    "    def get_beta(self):\n",
    "        return self.beta\n",
    "    \n",
    "    def set_probability(self, probability):\n",
    "        self.probability = probability\n",
    "    \n",
    "    def get_probability(self):\n",
    "        return self.probability\n",
    "    \n",
    "    def initial_state(self):\n",
    "        return [[0.0 for j in range(self.grid_size)] for i in range(self.grid_size)]\n",
    "    \n",
    "    def get_directions(self):\n",
    "        return self.directions\n",
    "    \n",
    "    def greedy_vk(self, state, actions):\n",
    "        \n",
    "        # store values to find argmax a\n",
    "        values = []\n",
    "        \n",
    "        # moves\n",
    "        moves = []\n",
    "        \n",
    "        # check neighbors in current state\n",
    "        for action in range(len(actions)):\n",
    "            \n",
    "            # get values for p(s',r|s,a)\n",
    "            s_prime, reward = self.move(state, actions[action])\n",
    "            \n",
    "            # compute q_pi(s,a)\n",
    "            value = reward + self.Gamma() * self.get_world()[s_prime[0]][s_prime[1]]\n",
    "            \n",
    "            # add to values\n",
    "            values.append(value)\n",
    "            \n",
    "            # add action to moves \n",
    "            moves.append(actions[action])\n",
    "        \n",
    "        # return argmax a\n",
    "        return moves[values.index(max(values))]\n",
    "\n",
    "    def move(self, state, action):\n",
    "        \"\"\"\n",
    "        State-Value Function Returns: State s', Reward r: from taking Action a in State s \n",
    "        q(s,a) in the Bellman Equation\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the +10 reward for A -> A' \n",
    "        if self.A == state:\n",
    "            return self.A_prime, self.A_goal\n",
    "        \n",
    "        # get the +5 reward for B -> B' \n",
    "        elif self.B == state:\n",
    "            return self.B_prime, self.B_goal\n",
    "        \n",
    "        # get the next state\n",
    "        state_prime = [state[0] + action[0], state[1] + action[1]]\n",
    "        \n",
    "        # row boundary\n",
    "        row_boundary = (state_prime[0] < 0 or state_prime[0] >= self.grid_size) \n",
    "        \n",
    "        # column boundary\n",
    "        column_boundary = (state_prime[1] < 0 or state_prime[1] >= self.grid_size) \n",
    "        \n",
    "        # check boundaries\n",
    "        if row_boundary or column_boundary:\n",
    "            self.reward = -1.0\n",
    "            state_prime = state\n",
    "        \n",
    "        # no reward in this state\n",
    "        else:\n",
    "            self.reward = 0\n",
    "            \n",
    "        return state_prime, self.reward\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman(grid_world):\n",
    "    \"\"\"\n",
    "    Policy Iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if bounded\n",
    "    bounded = False\n",
    "    \n",
    "    # set parameter vector of target policy\n",
    "    theta = 0.01\n",
    "    \n",
    "    # V(s) for all s of S\n",
    "    V = grid_world.initial_state()\n",
    "    \n",
    "    # iterate delta < theta \n",
    "    while not bounded:\n",
    "        \n",
    "        # set delta \n",
    "        delta = 0\n",
    "        \n",
    "        # set width for rows, cols\n",
    "        width = len(V)\n",
    "        \n",
    "        # set length iterate grid\n",
    "        length = width * width\n",
    "        \n",
    "        # iterate grid states\n",
    "        for i in range(length):\n",
    "            \n",
    "            # create row \n",
    "            row = i // width \n",
    "            \n",
    "            # create column\n",
    "            column = i % width\n",
    "            \n",
    "            # v <-- V(s)\n",
    "            v = V[row][column]\n",
    "            \n",
    "            # value for V(s)\n",
    "            value = 0\n",
    "            \n",
    "            # test all actions\n",
    "            for action in grid_world.directions.values():\n",
    "                \n",
    "                # get next state, reward\n",
    "                s_prime, reward = grid_world.move([row, column], action)\n",
    "\n",
    "                # computed by solving the system of linear equations BELLMAN EQUATION pi(a|s)p(s',r|s,a)[r + discount*Vpi(s)]\n",
    "                value += grid_world.get_probability() * (reward + grid_world.Gamma() * V[s_prime[0]][s_prime[1]])\n",
    "                \n",
    "            # add value to state\n",
    "            V[row][column] = value\n",
    "       \n",
    "            # absolute difference |v-V(S)|\n",
    "            delta = max(delta, abs(v - V[row][column]))\n",
    "                    \n",
    "        # loop until convergence\n",
    "        if delta < theta:\n",
    "            bounded = True\n",
    "   \n",
    "        # keep track of state values\n",
    "        grid_world.set_world(V.copy())\n",
    "        \n",
    "    # optimal policy\n",
    "    return grid_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_improvement(grid_world):\n",
    "    \"\"\"\n",
    "    Policy Improvement\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if bounded\n",
    "    bounded = False\n",
    "    \n",
    "    # set parameter vector of target policy\n",
    "    theta = 0.01\n",
    "    \n",
    "    # V(s) for all s of S\n",
    "    V = grid_world.initial_state()\n",
    "    \n",
    "    # iterate delta < theta \n",
    "    while not bounded:\n",
    "        \n",
    "        # set delta \n",
    "        delta = 0\n",
    "        \n",
    "        # grid with equiprobable random policy\n",
    "        v = grid_world.get_world().copy()\n",
    "        \n",
    "        # set width for rows, cols\n",
    "        width = len(v)\n",
    "        \n",
    "        # set length iterate grid\n",
    "        length = width * width\n",
    "        \n",
    "        # iterate grid states\n",
    "        for i in range(length):\n",
    "            \n",
    "            # create row \n",
    "            row = i // width \n",
    "            \n",
    "            # create column\n",
    "            column = i % width \n",
    "            \n",
    "            # store the action values for state\n",
    "            actions = {}\n",
    "            \n",
    "            # test all actions\n",
    "            for direction, action in grid_world.directions.items():\n",
    "                \n",
    "                # get next state, reward\n",
    "                s_prime, reward = grid_world.move([row, column], action)\n",
    "                \n",
    "                # UPDATE USING BELLMAN EQUATION\n",
    "                value = grid_world.get_probability() * (reward + grid_world.Gamma() * V[s_prime[0]][s_prime[1]])\n",
    "                \n",
    "                # update actions\n",
    "                actions[action] = value\n",
    "            \n",
    "            # place the optimal value in state\n",
    "            V[row][column] = sum(actions.values()) \n",
    "            \n",
    "            # update policy with optimal actions\n",
    "            grid_world.policy[(row, column)] = [action for action, value in actions.items() if value == max(actions.values())]\n",
    "            \n",
    "            # update policy with optimal values\n",
    "            grid_world.policy_values[(row, column)] = [value for action, value in actions.items() if value == max(actions.values()) and action in grid_world.policy[(row, column)]]\n",
    "      \n",
    "        # absolute difference |v-V(S)|\n",
    "        delta = max(delta, abs(sum(list(map(sum, v))) - sum(list(map(sum, V)))))\n",
    "        \n",
    "        # keep track of previous grid state values\n",
    "        grid_world.set_world(V)\n",
    "       \n",
    "        # loop until convergence\n",
    "        if  delta < theta:\n",
    "            bounded = True\n",
    "            \n",
    "        ######################\n",
    "        # Policy Improvement #\n",
    "        ######################\n",
    "        while bounded:\n",
    "\n",
    "            # policy stable <-- true\n",
    "            policy_stable = True\n",
    "\n",
    "            # iterate states of grid\n",
    "            for i in range(length):\n",
    "\n",
    "                # create row \n",
    "                row = i // width \n",
    "\n",
    "                # create column\n",
    "                column = i % width \n",
    "\n",
    "                # old action <-- pi(s)\n",
    "                old_action = choice(grid_world.policy[(row, column)])\n",
    "\n",
    "                # pi(S) <-- argmax a\n",
    "                new_action = grid_world.policy[(row, column)][grid_world.policy_values[(row, column)].index(max(grid_world.policy_values[(row, column)]))]\n",
    "\n",
    "                # if old action != pi(s) policy stable <-- false \n",
    "                if old_action != new_action:\n",
    "                    policy_stable = False\n",
    "                    grid_world.policy[(row, column)].remove(old_action)\n",
    "\n",
    "            # if policy stable return v* and pi* \n",
    "            if policy_stable:\n",
    "                bounded = True\n",
    "                return grid_world\n",
    "\n",
    "            # go back to Evaluation step\n",
    "            else:\n",
    "                bounded = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimal(grid_world):\n",
    "    \"\"\"\n",
    "    Value Iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if bounded\n",
    "    bounded = False\n",
    "    \n",
    "    # set parameter vector of target policy\n",
    "    theta = 0.0000001\n",
    "    \n",
    "    # V(s) for all s of S\n",
    "    V = grid_world.initial_state()\n",
    "    \n",
    "    # iterate delta < theta \n",
    "    while not bounded:\n",
    "        \n",
    "        # set delta \n",
    "        delta = 0\n",
    "        \n",
    "        # grid with zeros to keep track of difference\n",
    "        v = grid_world.get_world().copy()\n",
    "        \n",
    "        # set width for rows, cols\n",
    "        width = len(v)\n",
    "        \n",
    "        # set length it iterate 2d array\n",
    "        length = width * width\n",
    "        \n",
    "        # iterate grid\n",
    "        for i in range(length):\n",
    "            \n",
    "            # create row \n",
    "            row = i // width \n",
    "            \n",
    "            # create column\n",
    "            column = i % width \n",
    "            \n",
    "            # store the action values for state\n",
    "            actions = {}\n",
    "            \n",
    "            # test all actions\n",
    "            for action in grid_world.directions.values():\n",
    "                \n",
    "                # get next state, reward\n",
    "                s_prime, reward = grid_world.move([row, column], action)\n",
    "                \n",
    "                # UPDATE USING BELLMAN EQUATION\n",
    "                value = grid_world.get_probability() * (reward + grid_world.Gamma() * V[s_prime[0]][s_prime[1]])\n",
    "                \n",
    "                # update actions\n",
    "                actions[action] = value\n",
    "            \n",
    "            # place the optimal value in state\n",
    "            V[row][column] = max(actions.values()) \n",
    "            \n",
    "            # update policy with optimal actions\n",
    "            grid_world.policy[(row, column)] = [action for action, value in actions.items() if value == max(actions.values())]\n",
    "            \n",
    "            # update policy with optimal values\n",
    "            grid_world.policy_values[(row, column)] = [value for action, value in actions.items() if value == max(actions.values())]\n",
    "            \n",
    "        # absolute difference |v-V(S)|\n",
    "        delta = max(delta, abs(sum(list(map(sum, v))) - sum(list(map(sum, V)))))\n",
    "        \n",
    "        # loop until difference is less than parameter\n",
    "        if  delta < theta:\n",
    "            bounded = True\n",
    "            \n",
    "        # keep track of previous grid state values\n",
    "        grid_world.set_world(V)\n",
    "        \n",
    "    # optimal policy\n",
    "    return grid_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration: Evaluation Algorithm\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.362486775241769, 8.834659309360454, 4.471501905905386, 5.363176926234546, 1.533956921644943],\n",
      " [1.571000984726717, 3.035910592348995, 2.290863928710077, 1.946436976692719, 0.5859122065442595],\n",
      " [0.098722795581945, 0.7806930738022357, 0.7126059736710069, 0.39602772675179676, -0.36591308664296684],\n",
      " [-0.9260154548630697, -0.39332129185147424, -0.3157776047095224, -0.5481194156586933, -1.1462548418986562],\n",
      " [-1.8101295438052738, -1.3031313050747872, -1.1902602939304843, -1.385529460093907, -1.9384673291472465]]\n",
      "\n",
      "Policy Iteration: Improvement Algorithm\n",
      "{(0, 0): [(0, 1)],\n",
      " (0, 1): [(0, 1), (0, -1), (-1, 0), (1, 0)],\n",
      " (0, 2): [(0, -1)],\n",
      " (0, 3): [(0, 1), (0, -1), (-1, 0), (1, 0)],\n",
      " (0, 4): [(0, -1)],\n",
      " (1, 0): [(-1, 0)],\n",
      " (1, 1): [(-1, 0)],\n",
      " (1, 2): [(-1, 0)],\n",
      " (1, 3): [(-1, 0)],\n",
      " (1, 4): [(0, -1)],\n",
      " (2, 0): [(-1, 0)],\n",
      " (2, 1): [(-1, 0)],\n",
      " (2, 2): [(-1, 0)],\n",
      " (2, 3): [(-1, 0)],\n",
      " (2, 4): [(-1, 0)],\n",
      " (3, 0): [(-1, 0)],\n",
      " (3, 1): [(-1, 0)],\n",
      " (3, 2): [(-1, 0)],\n",
      " (3, 3): [(-1, 0)],\n",
      " (3, 4): [(-1, 0)],\n",
      " (4, 0): [(-1, 0)],\n",
      " (4, 1): [(-1, 0)],\n",
      " (4, 2): [(-1, 0)],\n",
      " (4, 3): [(-1, 0)],\n",
      " (4, 4): [(-1, 0)]}\n",
      "\n",
      "Value Iteration: Optimal Algorithm\n",
      "{(0, 0): [(0, 1)],\n",
      " (0, 1): [(0, 1), (0, -1), (-1, 0), (1, 0)],\n",
      " (0, 2): [(0, -1)],\n",
      " (0, 3): [(0, 1), (0, -1), (-1, 0), (1, 0)],\n",
      " (0, 4): [(0, -1)],\n",
      " (1, 0): [(0, 1), (-1, 0)],\n",
      " (1, 1): [(-1, 0)],\n",
      " (1, 2): [(0, -1), (-1, 0)],\n",
      " (1, 3): [(-1, 0)],\n",
      " (1, 4): [(0, -1), (-1, 0)],\n",
      " (2, 0): [(0, 1), (-1, 0)],\n",
      " (2, 1): [(-1, 0)],\n",
      " (2, 2): [(0, -1), (-1, 0)],\n",
      " (2, 3): [(-1, 0)],\n",
      " (2, 4): [(0, -1), (-1, 0)],\n",
      " (3, 0): [(0, 1), (-1, 0)],\n",
      " (3, 1): [(-1, 0)],\n",
      " (3, 2): [(0, -1), (-1, 0)],\n",
      " (3, 3): [(-1, 0)],\n",
      " (3, 4): [(0, -1), (-1, 0)],\n",
      " (4, 0): [(0, 1), (-1, 0)],\n",
      " (4, 1): [(-1, 0)],\n",
      " (4, 2): [(0, -1), (-1, 0)],\n",
      " (4, 3): [(-1, 0)],\n",
      " (4, 4): [(0, -1), (-1, 0)]}\n"
     ]
    }
   ],
   "source": [
    " if __name__ == '__main__':\n",
    "        \n",
    "        # create terminal printer instance\n",
    "        pp = pprint.PrettyPrinter(width=160, compact=True)\n",
    "        \n",
    "        # create the grid world object\n",
    "        grid_world = GridWorld(5, [0, 1], [0, 3], [4, 1], [2, 3], 10, 5, 0.9)\n",
    "\n",
    "        # Policy Iteration: Evaluation Algorithm\n",
    "        b = bellman(grid_world)\n",
    "        print(\"Policy Iteration: Evaluation Algorithm\")\n",
    "        sns.heatmap(b.get_world())\n",
    "        plt.show()\n",
    "        pp.pprint(b.get_world())\n",
    "        print()\n",
    "        \n",
    "        # Policy Iteration: Improvement Algorithm\n",
    "        b1 = bellman_improvement(grid_world)\n",
    "        print(\"Policy Iteration: Improvement Algorithm\")\n",
    "        pp.pprint(b1.get_policy())\n",
    "        print()\n",
    "        \n",
    "        # Value Iteration: Optimal Algorithm\n",
    "        b2 = bellman_optimal(grid_world)\n",
    "        print(\"Value Iteration: Optimal Algorithm\")\n",
    "        pp.pprint(b2.get_policy())\n",
    "        \n",
    "        # Arrows for plotting\n",
    "        a = [ '←', '↑', '→', '↓']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
