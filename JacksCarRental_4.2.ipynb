{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4.2: Jack’s Car Rental \n",
    "\n",
    "##### Jack manages two locations for a nationwide car rental company. \n",
    "- Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited \\\\$10 by the national company.\n",
    "- If he is out of cars at that location, then the business is lost. \n",
    "- Cars become available for renting the day after they are returned. \n",
    "- To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of \\\\$2 per car moved. \n",
    "\n",
    "We assume that the number of cars requested and returned at each\n",
    "location are Poisson random variables, meaning that the probability that the number is\n",
    "n is $\\frac{\\lambda^n}{n!}$ $e^{-\\lambda}$, where $\\lambda$ is the expected number. \n",
    "Suppose $\\lambda$ is 3 and 4 for rental requests at the first and second locations and 3 and 2 for returns. \n",
    "To simplify the problem slightly, we assume that there can be no more than 20 cars at each location \n",
    "(any additional cars are returned to the nationwide company, and thus disappear from the problem) and a\n",
    "maximum of five cars can be moved from one location to the other in one night. \n",
    "\n",
    "We take the discount rate to be $\\gamma$ = 0.9 and formulate this as a continuing finite MDP, where\n",
    "the time steps are days, the state is the number of cars at each location at the end of\n",
    "the day, and the actions are the net numbers of cars moved between the two locations\n",
    "overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4.2.png\">\n",
    "Figure 4.2: The sequence of policies found by policy iteration on Jack’s car rental problem,\n",
    "and the final state-value function. The first five diagrams show, for each number of cars at\n",
    "each location at the end of the day, the number of cars to be moved from the first location to\n",
    "the second (negative numbers indicate transfers from the second location to the first). Each\n",
    "successive policy is a strict improvement over the previous policy, and the last policy is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def backup_action(self, env, n1, n2, a):\n",
    "        \"\"\"\n",
    "        Return backup action\n",
    "        \"\"\"\n",
    "        \n",
    "        # Action a \n",
    "        a = max(-n2, min(a, n1))\n",
    "        \n",
    "        a = max(-5, min(5, a))\n",
    "        \n",
    "        # Value function expected return\n",
    "        value = -2 * abs(a)\n",
    "        \n",
    "        # Start of the day at location 1\n",
    "        morning_n1 = n1 - a\n",
    "        \n",
    "        # Start of the day at location 2\n",
    "        morning_n2 = n2 + a\n",
    "        \n",
    "        # Loop through states\n",
    "        for (new_n1, new_n2) in env.S:\n",
    "                \n",
    "            # Backup update\n",
    "            value += env.P1[morning_n1][new_n1] * env.P2[morning_n2][new_n2] * (\n",
    "                env.R1[morning_n1] + env.R2[morning_n2] + env.gamma*env.V[new_n1][new_n2])\n",
    "                \n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.size1 = 21\n",
    "        self.size2 = 26\n",
    "        self.V = [[0] * self.size1] * self.size1\n",
    "        self.lambda_requests1 = 3\n",
    "        self.lambda_requests2 = 4\n",
    "        self.lambda_dropoffs1 = 3\n",
    "        self.lambda_dropoffs2 = 2\n",
    "        self.policy = [[0] * self.size1] * self.size1\n",
    "        self.gamma = .9\n",
    "        self.theta = .000001\n",
    "        self.epsilon = 0.000000001\n",
    "        self.P1, self.R1 = self.load_P_and_R(self.lambda_requests1, self.lambda_dropoffs1)\n",
    "        self.P2, self.R2 = self.load_P_and_R(self.lambda_requests2, self.lambda_dropoffs2)\n",
    "        self.S = list({(i//self.size1, i%self.size1): i%self.size1 for i in range(self.size1*self.size1)}.keys())\n",
    "        \n",
    "    def factorial(self, n):\n",
    "        \"\"\"\n",
    "        Return factorial of n\n",
    "        \"\"\"\n",
    "        \n",
    "        # Base case\n",
    "        if n == 0: return 1\n",
    "        \n",
    "        # Recursive call\n",
    "        return n * self.factorial(n-1)\n",
    "    \n",
    "    def poisson (self, n, lambdas):\n",
    "        \"\"\"\n",
    "        The probability of n events according to the poisson distribution\n",
    "        \"\"\"\n",
    "        \n",
    "        return math.exp(-lambdas) * (lambdas**n/self.factorial(n))\n",
    "    \n",
    "    def load_P_and_R(self, lambda_requests, lambda_dropoffs):\n",
    "        \"\"\"\n",
    "        Load P(s'|s,a) and R(s,a)\n",
    "        \"\"\"\n",
    "        # Dynamics P\n",
    "        P = [[0] * self.size1] * self.size2\n",
    "        \n",
    "        # Rewards\n",
    "        R = [0] * self.size2\n",
    "        \n",
    "        # Requests and dropoffs per day\n",
    "        requests = dropoffs = 0\n",
    "        \n",
    "        # Request and dropoff probability\n",
    "        request_prob = drop_prob = self.theta\n",
    "        \n",
    "        # Load requests and dropoffs until probability is near certain\n",
    "        while request_prob >= self.theta:\n",
    "            \n",
    "            # Poisson random variable for request\n",
    "            request_prob = self.poisson(requests, lambda_requests)\n",
    "            \n",
    "            # Get transitions for requests\n",
    "            for n in range(self.size2):\n",
    "                \n",
    "                # Load reward for request\n",
    "                R[n] += 10 * request_prob * min(requests, n)\n",
    "            \n",
    "            # Load dropoffs until probability is near certain\n",
    "            while drop_prob >= self.theta:\n",
    "                \n",
    "                # Poisson random variable for dropoff\n",
    "                drop_prob = self.poisson(dropoffs, lambda_dropoffs)\n",
    "                    \n",
    "                # Get transitions for dropoffs\n",
    "                for n in range(self.size2):\n",
    "\n",
    "                    # Number of satisfied requests\n",
    "                    satisfied_requests = min(requests, n)\n",
    "\n",
    "                    # index Probability of s', r\n",
    "                    new_n = max(0, min(20, (n + dropoffs) - satisfied_requests))\n",
    "\n",
    "                    # Load Dynamic function P\n",
    "                    P[n][new_n] += request_prob * drop_prob\n",
    "                \n",
    "                # dropoffs\n",
    "                dropoffs += 1\n",
    "    \n",
    "            # Request\n",
    "            requests += 1\n",
    "                    \n",
    "        return P, R\n",
    "    \n",
    "    def pi(self, n1, n2, agent):\n",
    "        \"\"\"\n",
    "        Policy pi(a|s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Best value\n",
    "        best_value = 0\n",
    "        \n",
    "        # current values\n",
    "        this_value = float('-inf')\n",
    "        \n",
    "        # Best action in state\n",
    "        best_action = 0\n",
    "        \n",
    "        # Check range of actions\n",
    "        for a in range(max(-5, -n2), min(5, n1)+1):\n",
    "            \n",
    "            # Get the best value and action from state\n",
    "            while this_value <= best_value + self.epsilon:\n",
    "                \n",
    "                # Bellman backup update\n",
    "                this_value = agent.backup_action(self, n1, n2, a)\n",
    "            \n",
    "            # Best action value\n",
    "            best_value = this_value\n",
    "            \n",
    "            # Optimal action\n",
    "            best_action = a\n",
    "            \n",
    "        return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, agent):\n",
    "    \"\"\"\n",
    "    Return v_pi after policy evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Delta < theta condition\n",
    "    delta = env.theta\n",
    "    \n",
    "    # Loop until delta < theta\n",
    "    while delta >= env.theta:\n",
    "        \n",
    "        # Delta <-- 0\n",
    "        delta = 0\n",
    "        \n",
    "        # Loop for each s in S\n",
    "        for (s1, s2) in env.S:\n",
    "            \n",
    "            # v <-- V(s)\n",
    "            v = env.V[s1][s2]\n",
    "            \n",
    "            # Bellman update\n",
    "            env.V[s1][s2] = agent.backup_action(env, s1, s2, env.policy[s1][s2])\n",
    "            \n",
    "            # delta <-- max(delta, |v-V(s)|)\n",
    "            delta = max(delta, abs(v-env.V[s1][s2]))\n",
    "            #print(f\"d:{delta}, v:{v} - V:{env.V[s1][s2]} = {abs(v-env.V[s1][s2])}\")\n",
    "    return env.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedify(env, agent):\n",
    "    \"\"\"\n",
    "    Greedify Policy\n",
    "    \"\"\"\n",
    "\n",
    "    # Policy stable or improved \n",
    "    policy_improved = True\n",
    "    \n",
    "    # Loop for each s in S\n",
    "    for (n1, n2) in env.S:\n",
    "        \n",
    "        # behavior policy\n",
    "        b = env.policy[n1][n2]\n",
    "        \n",
    "        # Improve target policy pi \n",
    "        env.policy[n1][n2] = env.pi(n1, n2, agent)  \n",
    "        \n",
    "        # Check if policy stable\n",
    "        if b != env.policy[n1][n2]: policy_improved = False\n",
    "            \n",
    "    # policy is stable\n",
    "    return env.policy, policy_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, agent):\n",
    "    \"\"\"\n",
    "    Improve policy and return v* and pi*\n",
    "    \"\"\"\n",
    "    \n",
    "    # policy stable \n",
    "    policy_stable = False\n",
    "    \n",
    "    # Until stable\n",
    "    while not policy_stable:  \n",
    "            \n",
    "        # policy evaluation\n",
    "        env.V = policy_evaluation(env, agent)\n",
    "        \n",
    "        # policy improvement\n",
    "        env.policy, policy_stable = greedify(env, agent)\n",
    "    \n",
    "    # Policy stable return v* and pi*\n",
    "    return env.V, env.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4c84e3655ff9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     print(v)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#     print(pi)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-803b65eaec90>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[1;34m(env, agent)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# Bellman update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackup_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# delta <-- max(delta, |v-V(s)|)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-59c94fbac0a4>\u001b[0m in \u001b[0;36mbackup_action\u001b[1;34m(self, env, n1, n2, a)\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m# Backup update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             value += env.P1[morning_n1][new_n1] * env.P2[morning_n2][new_n2] * (\n\u001b[1;32m---> 26\u001b[1;33m                 env.R1[morning_n1] + env.R2[morning_n2] + env.gamma*env.V[new_n1][new_n2])\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Agent: Employee\n",
    "    agent = Agent()\n",
    "    \n",
    "    # Jack's Car Rental Environment\n",
    "    env = Environment()\n",
    "    \n",
    "    # Policy Iteration on Car Rental\n",
    "#     v, pi = policy_iteration(env, agent)\n",
    "    \n",
    "#     print(v)\n",
    "#     print(pi)\n",
    "    print(policy_evaluation(env, agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
