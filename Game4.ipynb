{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-size parameter --> (0,1]\n",
    "alpha = 0.1\n",
    "\n",
    "# Gamma\n",
    "gamma = 0.9\n",
    "\n",
    "# TERMINAL\n",
    "terminal = [0, 5]\n",
    "\n",
    "# n in n-step\n",
    "n = 2\n",
    "\n",
    "# Episodes (runs)\n",
    "episodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_td(alpha, gamma, n, terminal, episodes):\n",
    "    \"\"\"\n",
    "    n-step TD for estimating V = v_pi\n",
    "    \"\"\"\n",
    "\n",
    "    # Input: a policy pi\n",
    "    policy = 1\n",
    "    \n",
    "    # Initialize V(s) arbitrarily, for all s 2 S\n",
    "    V = [0] * 6\n",
    "    \n",
    "    # All store and access operations (for St and Rt) can take their index mod n + 1\n",
    "    store = []\n",
    "    \n",
    "    # Loop for each episode:\n",
    "    for i in range(episodes):\n",
    "        \n",
    "        # Initialize S0 != terminal\n",
    "        S = 1\n",
    "        \n",
    "        # Store S0 != terminal\n",
    "        store.append((S, 0))\n",
    "        \n",
    "        # T <-- inf\n",
    "        T = float('inf');  t = tau = 0\n",
    "        \n",
    "        # Loop for t = 0, 1, 2,... : Until t' = T - 1\n",
    "        while tau != T-1:\n",
    "\n",
    "            # If t < T, then:\n",
    "            if t < T:\n",
    "                \n",
    "                # Take an action according to pi(·|St)\n",
    "                S += policy\n",
    "                \n",
    "                # reset reward\n",
    "                R = 0\n",
    "                \n",
    "                # Take action A, observe R, S'\n",
    "                if S == terminal[0]: R = 0\n",
    "                    \n",
    "                # Take action A, observe R, S'\n",
    "                if S == terminal[1]: R = 1\n",
    "\n",
    "                # Store the next reward as Rt+1 and the next state as St+1\n",
    "                store.append((S, R))\n",
    "                \n",
    "                # If St+1 is terminal, then T <-- t + 1\n",
    "                if S in terminal: T = t + 1\n",
    "                print(T)\n",
    "            # t' <-- t - n +1 (t' is the time whose state’s estimate is being updated)\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If t' >= 0:\n",
    "            if tau >= 0:\n",
    "            \n",
    "                # G <-- sum(i=t'+1 to min(t'+n,T)) gamma^i-t'-1*Ri\n",
    "                G = sum([(gamma**(i-tau-1))*store[i][1] for i in range(tau + 1, min(tau + n, T)+1)])\n",
    "                print(f\"{t}: G={G} tau={tau}\")\n",
    "                # If t'+n < T, then: G <-- G + gamma^n*V(St'+n)\n",
    "                if tau+n < T: G = G + gamma**n*V[store[tau+n][0]]\n",
    "\n",
    "                # V(St') <-- V(St') + a[G - V(St')]\n",
    "                V[store[tau][0]] = V[store[tau][0]] + alpha*(G - V[store[tau][0]])\n",
    "            \n",
    "            # Until t' = T - 1\n",
    "            t += 1\n",
    "\n",
    "    # Return v_pi\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "inf\n",
      "1: G=0.0 tau=0\n",
      "inf\n",
      "2: G=0.0 tau=1\n",
      "4\n",
      "3: G=0.9 tau=2\n",
      "4: G=1.0 tau=3\n",
      "[0, 0.0, 0.0, 0.09000000000000001, 0.1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(n_step_td(alpha, gamma, n, terminal, episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_step_q_lol(alpha, gamma, n, terminal, episodes):\n",
    "    \"\"\"\n",
    "    n-step Q-learning Off-policy Learning\n",
    "    \"\"\"\n",
    "\n",
    "    # Input: a policy pi\n",
    "    policy = (-1, 1)\n",
    "    \n",
    "    # Initialize V(s) arbitrarily, for all s 2 S\n",
    "    V = [0] * 6; Q = {i: [0, 0] for i in range(6)}\n",
    "    \n",
    "    # All store and access operations (for St and Rt) can take their index mod n + 1\n",
    "    store = []\n",
    "    \n",
    "    # Loop for each episode:\n",
    "    for i in range(episodes):\n",
    "        \n",
    "        # Initialize S0 != terminal\n",
    "        S = 1\n",
    "        \n",
    "        # Store S0 != terminal\n",
    "        store.append((S, 0))\n",
    "        \n",
    "        # T <-- inf\n",
    "        T = float('inf');  t = tau = 0\n",
    "        \n",
    "        # Loop for t = 0, 1, 2,... : Until t' = T - 1\n",
    "        while tau != T-1:\n",
    "\n",
    "            # If t < T, then:\n",
    "            if t < T:\n",
    "                \n",
    "                # Take an action according to pi(·|St)\n",
    "                S += policy[1]\n",
    "                \n",
    "                # reset reward\n",
    "                R = 0\n",
    "                \n",
    "                # Take action A, observe R, S'\n",
    "                if S == terminal[0]: R = 0\n",
    "                    \n",
    "                # Take action A, observe R, S'\n",
    "                if S == terminal[1]: R = 1\n",
    "\n",
    "                # Store the next reward as Rt+1 and the next state as St+1\n",
    "                store.append((S, R))\n",
    "                \n",
    "                # If St+1 is terminal, then T <-- t + 1\n",
    "                if S in terminal: T = t + 1\n",
    "                print(T)\n",
    "            # t' <-- t - n +1 (t' is the time whose state’s estimate is being updated)\n",
    "            tau = t - n + 1\n",
    "            \n",
    "            # If t' >= 0:\n",
    "            if tau >= 0:\n",
    "            \n",
    "                # G <-- sum(i=t'+1 to min(t'+n,T)) gamma^i-t'-1*Ri\n",
    "                G = sum([(gamma**(i-tau-1))*store[i][1] for i in range(tau + 1, min(tau + n, T)+1)])\n",
    "                print(f\"{t}: G={G} tau={tau}\")\n",
    "                # If t'+n < T, then: G <-- G + gamma^n*max_aQ(St'+n, a)\n",
    "                if tau+n < T: G = G + gamma**n*max(Q[store[tau+n][0]])\n",
    "\n",
    "                # Q(St',At') <-- Q(St', At') + a[G - Q(St', At')]\n",
    "                Q[store[tau][0]][1] = Q[store[tau][0]][1] + alpha*(G - Q[store[tau][0]][1])\n",
    "            \n",
    "            # Until t' = T - 1\n",
    "            t += 1\n",
    "\n",
    "    # Return v_pi\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "inf\n",
      "1: G=0.0 tau=0\n",
      "inf\n",
      "2: G=0.0 tau=1\n",
      "4\n",
      "3: G=0.9 tau=2\n",
      "4: G=1.0 tau=3\n",
      "{0: [0, 0], 1: [0, 0.0], 2: [0, 0.0], 3: [0, 0.09000000000000001], 4: [0, 0.1], 5: [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(n_step_q_lol(alpha, gamma, n, terminal, episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(Q1, Q2, policy, state, epsilon):\n",
    "    \"\"\"\n",
    "    Return an action based on e-greedy policy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Exploitation Arg max a Q2\n",
    "    argmax = Q2[state].index(max(Q2[state]))\n",
    "    \n",
    "    # Q1 > Q2\n",
    "    if max(Q1[state]) > max(Q2[state]):\n",
    "        \n",
    "    \n",
    "        # Exploitation Arg max a\n",
    "        argmax = Q1[state].index(max(Q1[state]))\n",
    "    \n",
    "    # e-greedy action\n",
    "    action = policy[state][argmax]\n",
    "    \n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        \n",
    "        # Explorative action\n",
    "        action = random.choice(policy[state])\n",
    "    \n",
    "    # e-greedy action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action):\n",
    "    \"\"\"\n",
    "    Return environment obervation\n",
    "    \"\"\"\n",
    "\n",
    "    # S' after \n",
    "    s_prime = state + action\n",
    "    \n",
    "    # Check if terminal one reached\n",
    "    if s_prime == 0:\n",
    "        \n",
    "        # S', R = uniform distribution on termination\n",
    "        return s_prime, random.gauss(-0.1, 1.0)\n",
    "    \n",
    "    # S', R = 0 on all transitions and terminal two\n",
    "    return s_prime, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_Q_learning(episode, epsilon = 0.1):\n",
    "    \"\"\"\n",
    "    Return q* from Q-Learning Off-Policy TD Control (e-greedy)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###########################################\n",
    "    # Double Q-Learning for estimating Q = q* #\n",
    "    ###########################################\n",
    "    \n",
    "    # Policy and Terminal\n",
    "    policy = {0: [0, 0], 1: [-1, 1], 2: [-1, 1], 3: [0, 0]}; terminal = [0,3]\n",
    "    \n",
    "    # Algorithm parameters: step size a -> (0, 1], small \"a > 0\n",
    "    alpha = 0.1\n",
    "\n",
    "    # Initialize Q(s, a), for all s of S+, a of A(s), arbitrarily except that Q(terminal, ·)=0\n",
    "    Q1 = {0: [0, 0], 1: [0, 0], 2: [0, 0], 3: [0, 0]}; Q2 = {0: [0, 0], 1: [0, 0], 2: [0, 0], 3: [0, 0]} \n",
    "    \n",
    "    #Loop for each episode:\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        S = 2\n",
    "\n",
    "        #Loop for each step of episode: until S is terminal\n",
    "        while S not in terminal:\n",
    "            \n",
    "            # Choose A from S using policy derived from Q1 & Q2 (e-greedy)\n",
    "            A = e_greedy(Q1, Q2, policy, S, epsilon)\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            S_prime, R = move(S, A)\n",
    "            \n",
    "            # With 0.5 probabilility\n",
    "            if random.choice((0, 1)) == 1:\n",
    "                \n",
    "                # Q1(S, A) <-- Q1(S, A) + [R + gamma*Q2(S', argmax Q1(S', a)) - Q1(S, A)]\n",
    "                Q1[S][policy[S].index(A)] = Q1[S][policy[S].index(A)] + alpha*(R + Q2[S_prime][Q1[S_prime].index(max(Q1[S_prime]))]  - Q1[S][policy[S].index(A)])\n",
    "            \n",
    "            # else:\n",
    "            else:\n",
    "                \n",
    "                # Q2(S, A) <-- Q2(S, A) + [R + gamma*Q1(S', argmax Q2(S', a)) - Q2(S, A)]\n",
    "                Q2[S][policy[S].index(A)] = Q2[S][policy[S].index(A)] + alpha*(R + Q1[S_prime][Q2[S_prime].index(max(Q2[S_prime]))] - Q2[S][policy[S].index(A)])\n",
    "                \n",
    "            # S <-- S'\n",
    "            S = S_prime\n",
    "            \n",
    "    # Output Q estimate of q*\n",
    "    return list(Q1.values()), list(Q2.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
