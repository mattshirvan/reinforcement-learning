{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3.3 Recycling Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mobile robot has the job of collecting empty soda cans in an oce environment. It\n",
    "has sensors for detecting cans, and an arm and gripper that can pick them up and place\n",
    "them in an onboard bin; it runs on a rechargeable battery. The robot’s control system\n",
    "has components for interpreting sensory information, for navigating, and for controlling\n",
    "the arm and gripper. High-level decisions about how to search for cans are made by a\n",
    "reinforcement learning agent based on the current charge level of the battery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a simple example, we assume that only two charge levels can be distinguished, comprising\n",
    "a small state set S = {high, low}. In each state, the agent can decide whether to \n",
    "- (1) actively search for a can for a certain period of time, \n",
    "- (2) remain stationary and wait for someone to bring it a can, or \n",
    "- (3) head back to its home base to recharge its battery.\n",
    "\n",
    "When the energy level is high, recharging would always be foolish, so we do not include it\n",
    "in the action set for this state. The action sets are then A(high) = {search, wait} and\n",
    "A(low) = {search, wait, recharge}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rewards are zero most of the time, but become positive when the robot secures an\n",
    "empty can, or large and negative if the battery runs all the way down. The best way to\n",
    "find cans is to actively search for them, but this runs down the robot’s battery, whereas\n",
    "waiting does not. Whenever the robot is searching, the possibility exists that its battery\n",
    "will become depleted. In this case the robot must shut down and wait to be rescued\n",
    "(producing a low reward). If the energy level is high, then a period of active search can\n",
    "always be completed without risk of depleting the battery. A period of searching that\n",
    "begins with a high energy level leaves the energy level high with probability $\\alpha$ and reduces\n",
    "it to low with probability 1 - $\\alpha$. On the other hand, a period of searching undertaken\n",
    "when the energy level is low leaves it low with probability $\\beta$ and depletes the battery\n",
    "with probability 1 - $\\beta$. In the latter case, the robot must be rescued, and the battery is\n",
    "then recharged back to high. Each can collected by the robot counts as a unit reward,\n",
    "whereas a reward of -3 results whenever the robot has to be rescued. Let r$_{search}$ and\n",
    "r$_{wait}$, with r$_{search}$ > r$_{wait}$, respectively denote the expected number of cans the robot\n",
    "will collect (and hence the expected reward) while searching and while waiting. Finally,\n",
    "suppose that no cans can be collected during a run home for recharging, and that no cans\n",
    "can be collected on a step in which the battery is depleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"robot.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Model simulation of the office environment \n",
    "    for the Recycle Robot Example.\n",
    "    Includes random cans in the office at each run.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alpha = random.uniform(0.01, 0.5)\n",
    "        self.beta = random.uniform(0.01, 0.5)\n",
    "        self.high = 0\n",
    "        self.low = 1\n",
    "        self.search = 0\n",
    "        self.wait = 1\n",
    "        self.recharge = 2\n",
    "        self.searching = 1\n",
    "        self.waiting = 0.1\n",
    "        self.rescue = -3\n",
    "        self.S = [self.high, self.low]\n",
    "        self.A = [self.search, self.wait, self.recharge]\n",
    "        self.model = self.create_model()\n",
    "        self.R = self.create_rewards()\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create a tabular dynamics model for the simulated environment\n",
    "        Create action probabilities for p(s'|s,a)\n",
    "        model = dict<k,v> keys: states, values: array of action probability tuples\n",
    "        \"\"\"\n",
    "        \n",
    "        # model\n",
    "        model = {\n",
    "            self.high: [\n",
    "                (self.alpha, 1-self.alpha), \n",
    "                (0, 0),\n",
    "            ], \n",
    "            self.low: [\n",
    "                (self.beta, 1-self.beta), \n",
    "                (0, 0), \n",
    "                (1, 0)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def create_rewards(self):\n",
    "        \"\"\"\n",
    "        Create a tabular dynamics r(s,a,s') model for the simulated environment\n",
    "        \"\"\"\n",
    "        \n",
    "        # reward model\n",
    "        reward = {\n",
    "            self.high: [\n",
    "                (self.searching, self.searching), \n",
    "                (self.waiting, 0),\n",
    "            ], \n",
    "            self.low: [\n",
    "                (self.rescue, self.searching), \n",
    "                (0, self.waiting), \n",
    "                (0, 0)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def p(self, s, a, sprime):\n",
    "        \"\"\"\n",
    "        Returns Transition Probability p(s'|s,a)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.model[s][a][sprime]\n",
    "    \n",
    "    def r(self, s, a, sprime):\n",
    "        \"\"\"\n",
    "        Returns reward for s, a, s'\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.R[s][a][sprime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1, 1), (0.1, 0)], 1: [(-3, 1), (0, 0.1), (0, 0)]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825531514800546"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.p(0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pi = [[0.5, 0.33, 0], [0.33, 0.33, 0.33]]\n",
    "        self.gamma = 0.9\n",
    "    \n",
    "    def step(self, s, a, env, V):\n",
    "        \"\"\"\n",
    "        Mutates V from agent action\n",
    "        \"\"\"\n",
    "        \n",
    "        # Summation value\n",
    "        value = 0\n",
    "        \n",
    "        # Bootstrap next states from current (s,a)\n",
    "        for sprime in env.S:\n",
    "            \n",
    "            # Bellman Update\n",
    "            value += self.pi[s][a]*env.p(s, a, sprime)*(env.r(s,a,sprime) + self.gamma*V[sprime])\n",
    "        \n",
    "        return value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "policy = {0: [0, 1], 1: [0, 1, 2]}\n",
    "\n",
    "# Value function\n",
    "V = [0, 0]\n",
    "\n",
    "# Algorithm parameters\n",
    "delta = theta = 0.0001\n",
    "\n",
    "# Loop:\n",
    "while delta >= theta:\n",
    "    \n",
    "    # Delta <-- 0\n",
    "    delta = 0\n",
    "    \n",
    "    # Loop for each state\n",
    "    for s, actions in policy.items():\n",
    "        \n",
    "        # v <-- V(s)\n",
    "        v = V[s]\n",
    "        \n",
    "        # values for value function \n",
    "        value = 0\n",
    "        \n",
    "        # Test all actions\n",
    "        for a in actions:\n",
    "            \n",
    "            # Sum every action\n",
    "            value += agent.step(s, a, env, V)\n",
    "            \n",
    "        # Assign update to value function\n",
    "        V[s] = value\n",
    "        \n",
    "        # Delta = max(delta, |v-V(s)|)\n",
    "        delta = max(delta, abs(v-V[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "states = ['High', 'Low']\n",
    "values = V\n",
    "ax.bar(states, values)\n",
    "ax.set_title(\"Recycle Robot State Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
