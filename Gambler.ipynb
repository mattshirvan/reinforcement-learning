{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gambler’s Problem\n",
    "- A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. \n",
    "- If the coin comes up heads, gambler wins as many dollars as the stake on that flip\n",
    "- If the coin comes up tails, gambler loses the stake.\n",
    "- The game ends when the gambler wins by reaching goal of $100 or loses by running out of money\n",
    "- On each flip, the gambler must decide what portion of capital to stake, in integer numbers of dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object\n",
    "- Bet and win until capital = $100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Domain Analysis\n",
    " - Class Gambler: Environment\n",
    " - Game is modeled not built: Using either dynamics of P: S x A x S' x R  or experience through Monte Carlo Methods.\n",
    " - Game can be built out but is unnecessary for this problem set.\n",
    " - Compute max for V(s) and arg max a for deterministic policy Pi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gambler:\n",
    "    \n",
    "    def __init__(self, probability):\n",
    "        self.probability = probability\n",
    "        self.win_money = 1\n",
    "        self.lose_money = 1\n",
    "        self.goal = 100\n",
    "        self.reward = 0\n",
    "        self.policy, self.Returns, self.Return, self.Q = self.create()\n",
    "        self.V = [0.0 for i in range(self.goal+1)]\n",
    "        self.episode = []\n",
    "        self.converged = False\n",
    "        self.delta = 0\n",
    "        self.theta = 0.00000001\n",
    "        self.v = 0\n",
    "        self.gamma = 1.0\n",
    "        \n",
    "    def create(self):\n",
    "        \"\"\"\n",
    "        Creates arbitrary policy and returns list\n",
    "        \"\"\"\n",
    "        \n",
    "        # Empty structures for policy\n",
    "        policy = {}; returns = {}; r = {}; q = {}\n",
    "        \n",
    "        # Create policy and returns\n",
    "        for i in range(1, self.goal):\n",
    "   \n",
    "            # Policy - Key: states, Value: actions\n",
    "            policy[i] = [min(s, self.goal - s) for s in range(i+1)]\n",
    "            \n",
    "            # Returns - Key: states, Value: empty list for all s of S\n",
    "            returns[i] = []\n",
    "            \n",
    "            # Create states for Q(s,a)\n",
    "            q[i] = [min(s, self.goal - s) for s in range(i+1)]\n",
    "            \n",
    "        # Create Q(s, a) and Returns(s, a)\n",
    "        for state in policy:\n",
    "            \n",
    "            # Actions\n",
    "            for action in policy[state]:\n",
    "            \n",
    "                # Q(s,a) - Key: states, actions | Value: R \n",
    "                q[state][action] = 0.0\n",
    "\n",
    "                # Returns - Key: states, Value: empty list for all s of S\n",
    "                r[(state, action)] = []\n",
    "        \n",
    "        \n",
    "        return policy, returns, r, q\n",
    "        \n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        Output a deterministic policy pi = pi*\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############\n",
    "        # Initialize #\n",
    "        ##############\n",
    "        \n",
    "        # State value function\n",
    "        V = self.V.copy()\n",
    "        \n",
    "        # TERMINAL STATES\n",
    "        V[0] = V[-1] = 0\n",
    "        \n",
    "        # Arbitrary Policy\n",
    "        policy = self.policy.copy()\n",
    "        \n",
    "        #############################\n",
    "        # Loop until delta < theta: #\n",
    "        #############################\n",
    "        while not self.converged:\n",
    "            \n",
    "            # delta <- 0\n",
    "            self.delta = 0\n",
    "            \n",
    "            ########################\n",
    "            # loop for each s of S #\n",
    "            ########################\n",
    "            for state in self.policy:\n",
    "               \n",
    "                # keep track of actions, values\n",
    "                actions = {}\n",
    "                \n",
    "                # v <- V (s)\n",
    "                self.v = V[state]\n",
    "                \n",
    "                ##############\n",
    "                # GAME MODEL #\n",
    "                ##############\n",
    "                for action in self.policy[state]:\n",
    "                    \n",
    "                    # keep track of look ahead values\n",
    "                    self.win_money = state + action; self.lose_money = state - action\n",
    "                    \n",
    "                    # No reward during game or loss\n",
    "                    self.reward = r = 0\n",
    "                    \n",
    "                    # Reward for WIN\n",
    "                    if self.win_money >= self.goal:\n",
    "                        \n",
    "                        # Set value for V(s) index \n",
    "                        self.win_money = self.goal\n",
    "                        \n",
    "                        # Reward\n",
    "                        self.reward = 1\n",
    "                    \n",
    "                    # lose money\n",
    "                    if self.lose_money <= 0:\n",
    "                        \n",
    "                        # Set value for V(s) index\n",
    "                        self.lose_money = 0\n",
    "                    \n",
    "                    \n",
    "                    #############################################\n",
    "                    # V(s) max_a p(s', r|s, a)[r + Gamma*V(s')] #\n",
    "                    #############################################\n",
    "                    value = self.probability*(self.reward + self.gamma*(V[self.win_money])) + (1-self.probability)*(r + self.gamma*(V[self.lose_money]))\n",
    "\n",
    "                    # Track action values\n",
    "                    actions[action] = value\n",
    "                \n",
    "                # place the max value in state\n",
    "                V[state] = max(actions.values()) \n",
    "                \n",
    "                #######################################################\n",
    "                # Pi(s) = argmax_a SUM p(s', r|s, a)[r + Gamma*V(s')] #\n",
    "                #######################################################\n",
    "                policy[state] = [action for action, value in actions.items() if value == max(actions.values())]\n",
    "                \n",
    "                ###################################\n",
    "                # delta <- max(delta, |v - V(s)|) #\n",
    "                ###################################\n",
    "                self.delta = max(self.delta, abs(self.v - V[state]))\n",
    "                             \n",
    "            # keep track of states\n",
    "            self.V = V.copy()\n",
    "            \n",
    "            #######################\n",
    "            # until delta < theta #\n",
    "            #######################\n",
    "            if self.delta < self.theta:\n",
    "\n",
    "                # {Vk} converged to v*\n",
    "                self.converged = True\n",
    "        \n",
    "        ###########################################\n",
    "        # Output a deterministic policy, Pi = pi* #\n",
    "        ###########################################\n",
    "        self.policy = policy\n",
    "        return policy\n",
    "    \n",
    "    def first_visit(self, episodes):\n",
    "        \"\"\"\n",
    "        Returns Monte Carlo First Visit Prediction V = v*\n",
    "        \"\"\"\n",
    "        \n",
    "        #########################\n",
    "        # Loop For Each Episode #\n",
    "        #########################\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            #####################################################################\n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT #\n",
    "            #####################################################################\n",
    "            self.episode = [] \n",
    "            \n",
    "            # G <- 0\n",
    "            G = 0\n",
    "            \n",
    "            ######################\n",
    "            # GAME MODEL EPISODE #\n",
    "            ######################\n",
    "            \n",
    "            # loop for each s of S\n",
    "            for state in self.policy:\n",
    "            \n",
    "                # action in each state\n",
    "                for action in self.policy[state]:\n",
    "                    \n",
    "                    # keep track of win lose values\n",
    "                    self.win_money = state + action; self.lose_money = state - action\n",
    "                    \n",
    "                    # No reward during game or loss\n",
    "                    self.reward = 0\n",
    "                    \n",
    "                    # Reward for WIN\n",
    "                    if self.win_money >= self.goal:\n",
    "                        \n",
    "                        # Set value for index\n",
    "                        self.win_money = self.goal\n",
    "                        \n",
    "                        # Reward\n",
    "                        self.reward = 1\n",
    "                    \n",
    "                    # lose money\n",
    "                    if self.lose_money <= 0:\n",
    "                        \n",
    "                        # Set value for index\n",
    "                        self.lose_money = 0\n",
    "                        \n",
    "                    # Episode\n",
    "                    self.episode.append((state, action, self.reward))\n",
    "                    \n",
    "            ########################################################\n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0: #\n",
    "            ########################################################\n",
    "            for s, a, r in self.episode:\n",
    "                \n",
    "                # G <- Gamma*G + Rt+1\n",
    "                G = self.gamma*G + r\n",
    "              \n",
    "                # Unless St appears in S0, S1,..., St-1:\n",
    "                if s not in self.Returns:\n",
    "                    \n",
    "                    # Append G to Returns(St)\n",
    "                    self.Returns[s].append(G)\n",
    "\n",
    "                    # V(St) <- average(Returns(St))\n",
    "                    self.V[s] = sum(self.Returns[s])/len(self.Returns[s])\n",
    "\n",
    "        # V = v_pi\n",
    "        return self.V\n",
    "\n",
    "        \n",
    "    def exploration_starts(self, episodes):\n",
    "        \"\"\"\n",
    "        Returns Monte Carlo Exploration Starts Pi = pi*\n",
    "        \"\"\"\n",
    "        \n",
    "        # Policy pi\n",
    "        policy = self.policy.copy()\n",
    "        \n",
    "        # States\n",
    "        states = list(self.policy.keys())\n",
    "        \n",
    "        #########################\n",
    "        # Loop For Each Episode #\n",
    "        #########################\n",
    "        for i in range(episodes):\n",
    "            \n",
    "            # Episode history\n",
    "            episode = [] \n",
    "            \n",
    "            #####################################################################\n",
    "            # Generate an episode following p⇡: S0, A0, R1, ... ,ST-1, AT-1, RT #\n",
    "            #####################################################################\n",
    "            \n",
    "            # G <- 0\n",
    "            G = 0\n",
    "            \n",
    "            ######################\n",
    "            # GAME MODEL EPISODE #\n",
    "            ######################\n",
    "            \n",
    "            # Exploration starts\n",
    "            state = random.choice(states)\n",
    "\n",
    "            # loop for episode\n",
    "            while self.win_money < self.goal and self.lose_money > 0:\n",
    "                \n",
    "                # Random action in each state\n",
    "                action = random.choice(self.policy[state])\n",
    "                \n",
    "                # keep track of state - action values\n",
    "                self.win_money = state + action; self.lose_money = state - action\n",
    "\n",
    "                # No reward during game or loss\n",
    "                self.reward = 0\n",
    "\n",
    "                # Reward for WIN\n",
    "                if self.win_money >= self.goal:\n",
    "\n",
    "                    # Set value for index\n",
    "                    self.win_money = self.goal\n",
    "\n",
    "                    # Reward\n",
    "                    self.reward = 1\n",
    "\n",
    "                # lose money\n",
    "                if self.lose_money <= 0:\n",
    "\n",
    "                    # Set value for index\n",
    "                    self.lose_money = 0\n",
    "                    \n",
    "                    # consequence \n",
    "                    self.reward = -1\n",
    "\n",
    "                # Episode\n",
    "                episode.append((state, action, self.reward))\n",
    "                    \n",
    "            ########################################################\n",
    "            # Loop for each step of episode, t = T-1, T-2, ..., 0: #\n",
    "            ########################################################\n",
    "            for s, a, r in episode:\n",
    "                \n",
    "                # G <- Gamma*G + Rt+1\n",
    "                G = self.gamma*G + r\n",
    "              \n",
    "                # Unless St appears in S0, S1,..., St-1:\n",
    "                if s not in self.Return:\n",
    "                    \n",
    "                    # Append G to Returns(St, At)\n",
    "                    self.Return[(s, a)].append(G)\n",
    "\n",
    "                    # Q(St,At) <- average(Returns(St, At))\n",
    "                    self.Q[s][a] = sum(self.Return[(s, a)])/len(self.Return[(s,a)])\n",
    "                    \n",
    "                    # Pi(s) = argmax_a Q(St, a)\n",
    "                    policy[s] = [max(self.Q[s])]\n",
    "\n",
    "        # Pi = pi*\n",
    "        self.policy = policy\n",
    "        return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n",
      "{1: [0, 1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [0, 7], 8: [8], 9: [9], 10: [10], 11: [0, 11], 12: [12], 13: [0, 12, 13], 14: [11, 14], 15: [10, 15], 16: [0, 16], 17: [8, 17], 18: [0, 7, 18], 19: [0, 6, 19], 20: [5, 20], 21: [4], 22: [0, 3, 22], 23: [2], 24: [1, 24], 25: [0, 25], 26: [1, 24, 26], 27: [2, 23, 27], 28: [3], 29: [4], 30: [5, 20, 30], 31: [6, 19], 32: [7, 18], 33: [0, 17, 33], 34: [0, 9, 16, 34], 35: [10, 15], 36: [0, 11, 14], 37: [12, 13, 37], 38: [12], 39: [11], 40: [10], 41: [9], 42: [0, 8, 42], 43: [0, 7, 43], 44: [6, 44], 45: [5], 46: [0, 4, 46], 47: [3, 47], 48: [0, 2, 48], 49: [0, 1], 50: [0, 50], 51: [1, 49], 52: [2, 48], 53: [3], 54: [4, 46], 55: [5, 45], 56: [6, 44], 57: [43], 58: [0, 8], 59: [0, 9, 41], 60: [40], 61: [0, 39], 62: [12, 38], 63: [37], 64: [36], 65: [35], 66: [34], 67: [0, 8, 17, 33], 68: [0, 7, 18, 32], 69: [6, 19, 31], 70: [5, 30], 71: [0, 4, 21, 29], 72: [3, 22, 28], 73: [0, 2, 23, 27], 74: [0, 26], 75: [0, 25], 76: [1, 24], 77: [2], 78: [3, 22], 79: [0, 4, 21], 80: [5, 20], 81: [6], 82: [7, 18], 83: [8, 17], 84: [0, 9, 16], 85: [10, 15], 86: [11], 87: [0, 13], 88: [12], 89: [11], 90: [10], 91: [9], 92: [0, 8], 93: [7], 94: [6], 95: [5], 96: [0, 4], 97: [3], 98: [2], 99: [0, 1]}\n",
      "\n",
      "MC First Visit\n",
      "[0, 0.0020656234668683703, 0.00516405955015177, 0.009225470448896328, 0.012910153549983894, 0.017385398346403475, 0.023063677258169702, 0.027814111652448004, 0.03227538679658753, 0.037685072768208996, 0.04346349694116772, 0.050354469867537444, 0.057659194117779394, 0.06523937408012104, 0.0695352822693378, 0.0744312390078421, 0.08068846699146882, 0.08661104366092541, 0.09421268192052248, 0.10314362444807264, 0.10865874340470527, 0.11596662619655526, 0.1258861746688436, 0.13357997571793317, 0.1441479854307599, 0.16000000000000003, 0.16309843573009108, 0.16774609212999037, 0.17383820635490188, 0.17936523207795257, 0.18607809816470067, 0.19459551647066767, 0.20172116936160273, 0.20841308019488133, 0.2165276091523135, 0.2251952460428232, 0.2355317048013062, 0.24648879125845596, 0.2578590614380547, 0.2643029238129412, 0.27164685889882045, 0.28103270161696164, 0.2899165654913881, 0.3013190228807837, 0.3147154368628329, 0.3229881153392923, 0.33394993929483285, 0.3488292621176997, 0.36036996357689977, 0.3762219781461399, 0.4, 0.4030984357300911, 0.40774609212999036, 0.41383820635490187, 0.4193652320779525, 0.42607809816470066, 0.43459551647066763, 0.4417211693616027, 0.44841308019488135, 0.4565276091523135, 0.4651952460428232, 0.4755317048013062, 0.48648879125845595, 0.4978590614380547, 0.5043029238129412, 0.5116468588988204, 0.5210327016169617, 0.5299165654913881, 0.5413190228807837, 0.5547154368628329, 0.5629881153392923, 0.5739499392948328, 0.5888292621176997, 0.6003699635768998, 0.6162219781461399, 0.64, 0.6446476552779943, 0.6516191392467716, 0.6607573098824007, 0.6690478481169289, 0.6791171476256939, 0.6918932747550737, 0.7025817542877647, 0.712619620970177, 0.7247914137284702, 0.7377928692035753, 0.7532975572706199, 0.769733186887684, 0.7867885931667966, 0.7964543859294404, 0.8074702885754164, 0.8215490525726588, 0.8348748482370821, 0.8519785343623719, 0.872073155900078, 0.8844821731452498, 0.9009249089422493, 0.9232438935400468, 0.9405549453653496, 0.9643329672192097, 0]\n",
      "\n",
      "MC Exploration Starts\n",
      "{1: [0, 1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [0, 7], 8: [8], 9: [9], 10: [10], 11: [0, 11], 12: [12], 13: [0, 12, 13], 14: [11, 14], 15: [10, 15], 16: [0, 16], 17: [8, 17], 18: [0, 7, 18], 19: [0, 6, 19], 20: [5, 20], 21: [4], 22: [0, 3, 22], 23: [2], 24: [1, 24], 25: [0, 25], 26: [1, 24, 26], 27: [2, 23, 27], 28: [3], 29: [4], 30: [5, 20, 30], 31: [6, 19], 32: [7, 18], 33: [0, 17, 33], 34: [0, 9, 16, 34], 35: [10, 15], 36: [0, 11, 14], 37: [12, 13, 37], 38: [12], 39: [11], 40: [10], 41: [9], 42: [0, 8, 42], 43: [0, 7, 43], 44: [6, 44], 45: [5], 46: [0, 4, 46], 47: [3, 47], 48: [0, 2, 48], 49: [0, 1], 50: [0, 50], 51: [1, 49], 52: [2, 48], 53: [3], 54: [4, 46], 55: [5, 45], 56: [6, 44], 57: [43], 58: [0, 8], 59: [0, 9, 41], 60: [40], 61: [0, 39], 62: [12, 38], 63: [37], 64: [36], 65: [35], 66: [34], 67: [0, 8, 17, 33], 68: [0, 7, 18, 32], 69: [6, 19, 31], 70: [5, 30], 71: [0, 4, 21, 29], 72: [3, 22, 28], 73: [0, 2, 23, 27], 74: [0, 26], 75: [0, 25], 76: [1, 24], 77: [2], 78: [3, 22], 79: [0, 4, 21], 80: [5, 20], 81: [6], 82: [7, 18], 83: [8, 17], 84: [0, 9, 16], 85: [10, 15], 86: [11], 87: [0, 13], 88: [12], 89: [11], 90: [10], 91: [9], 92: [0, 8], 93: [7], 94: [6], 95: [5], 96: [0, 4], 97: [3], 98: [2], 99: [0, 1]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    g = Gambler(0.4)\n",
    "    \n",
    "    print(\"Value Iteration\")\n",
    "    print(g.value_iteration())\n",
    "    print()\n",
    "    print(\"MC First Visit\")\n",
    "    print(g.first_visit(10000))\n",
    "    print()\n",
    "    print(\"MC Exploration Starts\")\n",
    "    print(g.exploration_starts(10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
