{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes \n",
    "episodes = 170\n",
    "\n",
    "# Start\n",
    "start = (3, 0)\n",
    "\n",
    "# Goal\n",
    "goal = (3, 11)\n",
    "\n",
    "# Epsilon for e-greedy\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create():\n",
    "    \"\"\"\n",
    "    Create and return Q and arbitrary policy\n",
    "    \"\"\"\n",
    "    \n",
    "    # Width of Grid\n",
    "    width = 4\n",
    "\n",
    "    # Create length to iterate grid\n",
    "    length = 12\n",
    "\n",
    "    # Set of equiprobable actions\n",
    "    actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    \n",
    "    # Policy and Q\n",
    "    policy = {}; Q = {}\n",
    "\n",
    "    # Create states for policy\n",
    "    for i in range(length*width):\n",
    "\n",
    "        # Create row index\n",
    "        row = i // length\n",
    "\n",
    "        # Create column index\n",
    "        column = i % length\n",
    "\n",
    "        # Create policy\n",
    "        policy[(row, column)] = actions\n",
    "        \n",
    "        # Create Q(s,a)\n",
    "        Q[(row, column)] = [0] * len(actions)\n",
    "\n",
    "    # Q and policy \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary(state):\n",
    "    \"\"\"\n",
    "    Returns if agent is in bounds\n",
    "    \"\"\"\n",
    "    # row boundary\n",
    "    row_boundary = (state[0] < 0 or state[0] >= 4) \n",
    "        \n",
    "    # column boundary\n",
    "    column_boundary = (state[1] < 0 or state[1] >= 12) \n",
    "    \n",
    "    # if not in bounds\n",
    "    return row_boundary or column_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(Q, policy, state, goal, epsilon):\n",
    "    \"\"\"\n",
    "    Return an action based on e-greedy policy\n",
    "    \"\"\"\n",
    "    \n",
    "    # state is goal or out of bounds\n",
    "    if state == goal or boundary(state):\n",
    "        \n",
    "        # Do not move\n",
    "        return (0,0)\n",
    "\n",
    "    # Exploitation Arg max a\n",
    "    argmax = Q[state].index(max(Q[state]))\n",
    "    \n",
    "    # e-greedy action\n",
    "    action = policy[state][argmax]\n",
    "    \n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        \n",
    "        # Explorative action\n",
    "        action = random.choice(policy[state])\n",
    "    \n",
    "    # e-greedy action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action, goal, start):\n",
    "    \"\"\"\n",
    "    Return environment obervation\n",
    "    \"\"\"\n",
    "    \n",
    "    #########\n",
    "    # CLIFF #\n",
    "    #########\n",
    "    cliff = [(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10)]\n",
    "    \n",
    "    # S' after stochastic wind\n",
    "    s_prime = (state[0] + action[0], state[1] + action[1])\n",
    "    \n",
    "    # Check goal reached\n",
    "    if s_prime == goal:\n",
    "        \n",
    "        # S', R = 0 on termination\n",
    "        return s_prime, 0\n",
    "    \n",
    "    # Agent Fell off cliff\n",
    "    if s_prime in cliff:\n",
    "        \n",
    "        # Back to the start\n",
    "        return start, -100\n",
    "    \n",
    "    # check if out of bounds\n",
    "    if boundary(s_prime):\n",
    "        \n",
    "        # Remain in state\n",
    "        s_prime = state\n",
    "    \n",
    "    # S', R = -1 on all transitions\n",
    "    return s_prime, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_on_policy(episodes, start, goal, epsilon):\n",
    "    \"\"\"\n",
    "    Return q* from SARSA On-Policy (e-greedy)\n",
    "    \"\"\"\n",
    "    \n",
    "    ########################################################\n",
    "    # Sarsa (on-policy TD control) for estimating Q = q_pi #\n",
    "    ########################################################\n",
    "\n",
    "    # Algorithm parameters: step size a -> (0, 1], small \"a > 0\n",
    "    alpha = 0.5\n",
    "\n",
    "    # Initialize Q(s, a), for all s of S+, a of A(s), arbitrarily except that Q(terminal, ·)=0\n",
    "    Q, policy = create()\n",
    "\n",
    "    #Loop for each episode:\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        S = start\n",
    "\n",
    "        # Choose A from S using policy derived from Q (e-greedy)\n",
    "        A = e_greedy(Q, policy, S, goal, epsilon)\n",
    "\n",
    "        #Loop for each step of episode: until S is terminal\n",
    "        while S != goal:\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            S_prime, R = move(S, A, goal, start)\n",
    "            \n",
    "            # Choose A' from S' using policy derived from Q (e-greedy)\n",
    "            A_prime = e_greedy(Q, policy, S_prime, goal, epsilon)\n",
    "\n",
    "            # Q(S, A) <-- Q(S, A) + [R + gamma*Q(S', A') - Q(S, A)]\n",
    "            Q[S][policy[S].index(A)] = Q[S][policy[S].index(A)] + alpha*(R + Q[S_prime][policy[S_prime].index(A_prime)] - Q[S][policy[S].index(A)])\n",
    "            \n",
    "            # S <-- S'; A <-- A';\n",
    "            S = S_prime; A = A_prime\n",
    "            \n",
    "    # Output Q estimate of q*\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(episodes, start, goal, epsilon):\n",
    "    \"\"\"\n",
    "    Return q* from Q-Learning Off-Policy TD Control (e-greedy)\n",
    "    \"\"\"\n",
    "    \n",
    "    ##########################################################\n",
    "    # Q-Learning Off-Policy TD Control for estimating Q = q* #\n",
    "    ##########################################################\n",
    "\n",
    "    # Algorithm parameters: step size a -> (0, 1], small \"a > 0\n",
    "    alpha = 0.5\n",
    "\n",
    "    # Initialize Q(s, a), for all s of S+, a of A(s), arbitrarily except that Q(terminal, ·)=0\n",
    "    Q, policy = create()\n",
    "\n",
    "    #Loop for each episode:\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        S = start\n",
    "\n",
    "        #Loop for each step of episode: until S is terminal\n",
    "        while S != goal:\n",
    "            \n",
    "            # Choose A from S using policy derived from Q (e-greedy)\n",
    "            A = e_greedy(Q, policy, S, goal, epsilon)\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            S_prime, R = move(S, A, goal, start)\n",
    "\n",
    "            # Q(S, A) <-- Q(S, A) + [R + gamma*Q(S', A') - Q(S, A)]\n",
    "            Q[S][policy[S].index(A)] = Q[S][policy[S].index(A)] + alpha*(R + max(Q[S_prime]) - Q[S][policy[S].index(A)])\n",
    "            \n",
    "            # S <-- S'\n",
    "            S = S_prime\n",
    "            \n",
    "    # Output Q estimate of q*\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(episodes, start, goal, epsilon):\n",
    "    \"\"\"\n",
    "    Return q* from Expected Sarsa TD Control (e-greedy)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###################################################\n",
    "    # Expected Sarsa TD Control for estimating Q = q* #\n",
    "    ###################################################\n",
    "\n",
    "    # Algorithm parameters: step size a -> (0, 1], small \"a > 0\n",
    "    alpha = 0.5\n",
    "\n",
    "    # Initialize Q(s, a), for all s of S+, a of A(s), arbitrarily except that Q(terminal, ·)=0\n",
    "    Q, policy = create()\n",
    "\n",
    "    #Loop for each episode:\n",
    "    for _ in range(episodes):\n",
    "\n",
    "        # Initialize S\n",
    "        S = start\n",
    "\n",
    "        #Loop for each step of episode: until S is terminal\n",
    "        while S != goal:\n",
    "            \n",
    "            # Choose A from S using policy derived from Q (e-greedy)\n",
    "            A = e_greedy(Q, policy, S, goal, epsilon)\n",
    "\n",
    "            # Take action A, observe R, S'\n",
    "            S_prime, R = move(S, A, goal, start)\n",
    "\n",
    "            # Q(S, A) <-- Q(S, A) + [R + gamma*pi(a|St+1)*Q(St+1, a)  Q(St, At)] includes bootstrapping \n",
    "            Q[S][policy[S].index(A)] = Q[S][policy[S].index(A)] + alpha*(R + ((1-epsilon)*max(Q[S_prime]) + epsilon/len(policy[S_prime])*sum(Q[S_prime])) - Q[S][policy[S].index(A)])\n",
    "            \n",
    "            # S <-- S'\n",
    "            S = S_prime\n",
    "            \n",
    "    # Output Q estimate of q*\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning\n",
      "{(0, 0): [-9.921661376953125, -10.423448341756739, -9.923095703125, -10.118081634747796], (0, 1): [-9.9388427734375, -10.074306839224846, -10.091433328267158, -9.762237860122696], (0, 2): [-9.063621520996094, -9.127956374548376, -9.651172772690188, -9.036303657921962], (0, 3): [-8.28125, -8.799790023360401, -9.040111972950399, -8.371845092624426], (0, 4): [-7.8125, -8.003346993820742, -8.242163620889187, -7.744209345430136], (0, 5): [-7.0, -6.958580732345581, -8.080220684409142, -6.9318785555660725], (0, 6): [-6.5, -6.305429560656194, -7.114945277571678, -6.214316293597221], (0, 7): [-5.5, -5.508627311606691, -5.44888973236084, -5.491674304008484], (0, 8): [-4.71875, -4.727063780534081, -4.9317779541015625, -4.696332883089781], (0, 9): [-4.0, -3.8745597056112855, -4.359535217285156, -3.8137969970703125], (0, 10): [-3.0, -2.939502716064453, -3.4501953125, -2.90887451171875], (0, 11): [-2.0, -1.988433837890625, -2.59375, -2.25], (1, 0): [-10.518830832530057, -10.529373372980423, -11.0, -10.717195079843805], (1, 1): [-10.283463532447058, -10.341937817636275, -10.040877051083953, -9.982935310992616], (1, 2): [-9.349854421801865, -9.36030863362248, -9.565687969770806, -9.368928717907693], (1, 3): [-8.591341838240623, -8.7332346938683, -8.649352677166462, -8.465807214284723], (1, 4): [-8.23793775588274, -7.805261473158967, -8.324425893544685, -7.652929133452949], (1, 5): [-7.149660386145115, -6.850296799828357, -7.201788917183876, -6.803702889730129], (1, 6): [-6.617716535925865, -5.930637649056678, -6.386617809534073, -5.931737934127058], (1, 7): [-5.090492248535156, -4.979535120985427, -5.78054616227746, -4.980068143540102], (1, 8): [-5.120368480682373, -3.9918473678644677, -4.2822265625, -3.9921302845149764], (1, 9): [-3.85693359375, -2.998982128454356, -4.387331485748291, -2.997965890861451], (1, 10): [-2.775390625, -1.9995038434499293, -2.234375, -1.9997494220733643], (1, 11): [-2.234375, -0.9999998807907104, -2.060515373799717, -1.4999847412109375], (2, 0): [-11.323380324339642, -12.652403337210698, -11.98872868512441, -11.0], (2, 1): [-10.562950210787676, -111.40409248270464, -11.260544064901843, -10.0], (2, 2): [-10.10158333983236, -103.66711415495453, -10.579572881680258, -9.0], (2, 3): [-8.746966625913046, -111.95876481670835, -9.804036579330983, -8.0], (2, 4): [-8.29346699240341, -110.94967465408608, -8.968703402721518, -7.0], (2, 5): [-6.2327752113342285, -108.0011478149581, -7.684828747063875, -6.0], (2, 6): [-6.825421266170395, -108.21339796123479, -6.580894947051935, -5.0], (2, 7): [-5.900513277640888, -109.36287743976396, -5.9973999261854285, -4.0], (2, 8): [-4.638447192715834, -110.10438847494976, -4.922561486748046, -3.0], (2, 9): [-3.944753064621149, -104.44967693937494, -3.9846315335679527, -2.0], (2, 10): [-2.4910658597946167, -104.3547198927856, -2.9445037841796875, -1.0], (2, 11): [-1.9062482118606567, 0.0, -1.6874999701976705, -0.9921875], (3, 0): [-12.0, -12.994679946359007, -12.997365603042962, -111.85681397511298], (3, 1): [0, 0, 0, 0], (3, 2): [0, 0, 0, 0], (3, 3): [0, 0, 0, 0], (3, 4): [0, 0, 0, 0], (3, 5): [0, 0, 0, 0], (3, 6): [0, 0, 0, 0], (3, 7): [0, 0, 0, 0], (3, 8): [0, 0, 0, 0], (3, 9): [0, 0, 0, 0], (3, 10): [0, 0, 0, 0], (3, 11): [0, 0, 0, 0]}\n",
      "\n",
      "Expected-Sarsa\n",
      "{(0, 0): [-12.655039429150282, -12.496480075005696, -12.755089416172748, -12.651876973885056], (0, 1): [-12.186642732496171, -12.436768450836443, -12.189937382397364, -11.870622055272978], (0, 2): [-11.199833432528283, -11.406127279908548, -11.704981389696517, -11.04259345135671], (0, 3): [-10.20114298863563, -10.624964264785389, -11.338293383093013, -10.121827018078246], (0, 4): [-9.482610806492032, -9.366397666723874, -9.944396211507716, -9.173085658706718], (0, 5): [-8.617576119715249, -8.438326812426086, -8.515592612769433, -8.213880201284105], (0, 6): [-7.915275142033149, -7.410023124727705, -7.98846813923107, -7.233863667888257], (0, 7): [-6.583514203659806, -6.312097683411308, -6.6024384140064765, -6.247591229165414], (0, 8): [-5.461290967223117, -5.366764618120659, -5.634185994368824, -5.262074397918865], (0, 9): [-4.577433129329634, -4.2899225264480885, -4.7678561597393845, -4.2641466267839725], (0, 10): [-3.5518593101555522, -3.283647443705893, -3.7910661244821267, -3.252931469970397], (0, 11): [-2.452926852285689, -2.240315483927179, -2.561494135481378, -2.5702366537938417], (1, 0): [-13.298201592267752, -14.790668289935045, -14.06262128934318, -13.22139121054077], (1, 1): [-12.138021590143222, -14.455371748236917, -14.172513802445078, -12.116795435530056], (1, 2): [-11.822308925968297, -11.249195397089263, -12.907954778453107, -11.052744716479774], (1, 3): [-11.100391579624688, -10.531880315445541, -11.231420503482944, -9.994230728939653], (1, 4): [-9.96754142965165, -11.4446541019906, -10.794817438670513, -8.865090334280467], (1, 5): [-8.976831209714451, -10.220726539726414, -9.433168207676658, -7.740393100071001], (1, 6): [-8.03165230898851, -8.479215725637848, -7.5709150839751596, -6.656652195015772], (1, 7): [-7.000121082465725, -5.856287393991536, -7.11039326199847, -5.590993001835992], (1, 8): [-5.438956416662506, -7.479414360432778, -5.434082652168008, -4.475104492299339], (1, 9): [-4.331527775366002, -6.206723244011505, -5.145878045182151, -3.338283355111734], (1, 10): [-3.919879069500827, -2.318896519165527, -4.250747252333884, -2.2466051282405672], (1, 11): [-3.101464146381292, -1.1461107649240176, -2.2076521420209367, -2.168737557434619], (2, 0): [-14.278462805564375, -18.549900454872954, -15.256042990490716, -15.290611383967404], (2, 1): [-12.91118500862996, -82.02371199566034, -13.829053976053022, -12.941613806459042], (2, 2): [-11.499187754230011, -51.44158003138505, -11.674196426448177, -11.824577333278068], (2, 3): [-10.33173822731911, -81.61918860270757, -10.757249957980992, -9.830874724779095], (2, 4): [-9.745565614727335, -51.979900279632474, -10.10120724154993, -10.309283866363273], (2, 5): [-8.582949200831454, -79.15040688476796, -9.104469575699717, -8.793820786064982], (2, 6): [-7.237602424509127, -51.35605108297338, -8.96443909070556, -7.156610658476616], (2, 7): [-5.795702963386928, -51.98567119403036, -6.02903495768804, -6.681429832125802], (2, 8): [-5.4467473476375625, -52.22871876872669, -6.397787580220797, -5.709389428191316], (2, 9): [-3.671303375808669, -99.51598754710457, -4.497317391701978, -3.3391746004482865], (2, 10): [-1.6956478933638048, -52.522467170641825, -1.4193851080300406, -1.1294663167609191], (2, 11): [-1.7906326164606134, 0.0, -2.929493374939539, -1.126347890814157], (3, 0): [-15.430248065116999, -18.66519728858549, -18.46837916968373, -117.83679949073768], (3, 1): [0, 0, 0, 0], (3, 2): [0, 0, 0, 0], (3, 3): [0, 0, 0, 0], (3, 4): [0, 0, 0, 0], (3, 5): [0, 0, 0, 0], (3, 6): [0, 0, 0, 0], (3, 7): [0, 0, 0, 0], (3, 8): [0, 0, 0, 0], (3, 9): [0, 0, 0, 0], (3, 10): [0, 0, 0, 0], (3, 11): [0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-Learning\")\n",
    "print(Q_learning(episodes, start, goal, epsilon))\n",
    "print()\n",
    "print(\"Expected-Sarsa\")\n",
    "print(expected_sarsa(episodes, start, goal, epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
